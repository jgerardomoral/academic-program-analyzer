# Plan Final Completo: An√°lisis de Programas de Estudio con Streamlit

## Estructura Final del Proyecto

```
analisis-programas-estudio/
‚îÇ
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ config.toml              # Configuraci√≥n de Streamlit
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                     # PDFs originales
‚îÇ   ‚îú‚îÄ‚îÄ processed/               # Textos extra√≠dos (JSON/pickle)
‚îÇ   ‚îú‚îÄ‚îÄ cache/                   # Cache de an√°lisis
‚îÇ   ‚îî‚îÄ‚îÄ taxonomia/
‚îÇ       ‚îú‚îÄ‚îÄ habilidades.json     # Taxonom√≠a de habilidades
‚îÇ       ‚îî‚îÄ‚îÄ stopwords_custom.txt # Stopwords personalizadas
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ extraction/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_extractor.py    # Extracci√≥n con pdfplumber
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessor.py     # Limpieza y normalizaci√≥n
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ analysis/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frequency.py        # TF-IDF, n-grams
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ topics.py           # LDA/NMF
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ skills_mapper.py    # Mapeo keywords ‚Üí habilidades
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ visualization/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ plotly_charts.py    # Gr√°ficos interactivos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wordclouds.py       # WordClouds
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reports.py          # Generaci√≥n de reportes HTML
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py           # Carga/gesti√≥n de config.yaml
‚îÇ       ‚îú‚îÄ‚îÄ file_manager.py     # I/O de archivos
‚îÇ       ‚îî‚îÄ‚îÄ validators.py       # Validaci√≥n de inputs
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ Home.py                 # P√°gina principal (entry point)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1_üìÅ_Subir_PDFs.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2_üìä_Analisis_Frecuencias.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 3_üéØ_Topics_Habilidades.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 4_üìà_Comparativa.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 5_‚öôÔ∏è_Configuracion.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sidebar.py          # Sidebar com√∫n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filters.py          # Widgets de filtrado
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ charts.py           # Wrappers de visualizaciones
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tables.py           # Tablas interactivas
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ session_manager.py  # Gesti√≥n de st.session_state
‚îÇ       ‚îú‚îÄ‚îÄ cache_manager.py    # Decoradores de cach√©
‚îÇ       ‚îî‚îÄ‚îÄ export.py           # Exportaci√≥n de datos
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01_exploracion_pdfs.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_pruebas_nlp.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_topic_modeling.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 04_validacion_habilidades.ipynb
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_extraction.py
‚îÇ   ‚îú‚îÄ‚îÄ test_analysis.py
‚îÇ   ‚îî‚îÄ‚îÄ test_app_components.py
‚îÇ
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ reports/                # Reportes HTML/PDF
‚îÇ   ‚îî‚îÄ‚îÄ exports/                # CSV/Excel exportados
‚îÇ
‚îú‚îÄ‚îÄ .env.example                # Variables de entorno
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ config.yaml                 # Configuraci√≥n principal
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ docker-compose.yml          # (Opcional) Para deploy
```

---

## Estrategia de Desarrollo en Paralelo con Claude Code

### **Principio Clave: Modularidad con Contratos Claros**

Para trabajar en paralelo, cada m√≥dulo debe tener:
1. **Input/Output definido** (tipos, formatos)
2. **Tests unitarios** como contrato
3. **Datos mock** para desarrollo independiente
4. **Documentaci√≥n inline** con ejemplos

---

## FASE 0: Setup Inicial (1 d√≠a) - **CR√çTICO PARA PARALELIZACI√ìN**

### **Tareas:**
1. ‚úÖ Crear estructura de directorios completa
2. ‚úÖ Configurar `requirements.txt` y entorno virtual
3. ‚úÖ Crear `config.yaml` base
4. ‚úÖ Definir **contratos de datos** (schemas)
5. ‚úÖ Crear datos mock para desarrollo
6. ‚úÖ Setup de tests b√°sicos

### **Archivos a crear:**

#### `requirements.txt`
```txt
# Core
python>=3.10

# PDF Processing
pdfplumber>=0.10.0
PyPDF2>=3.0.0
tabula-py>=2.8.0

# NLP
spacy>=3.7.0
nltk>=3.8.0
scikit-learn>=1.3.0
gensim>=4.3.0

# Data Processing
pandas>=2.1.0
numpy>=1.25.0
scipy>=1.11.0

# Visualization
matplotlib>=3.8.0
seaborn>=0.13.0
plotly>=5.18.0
wordcloud>=1.9.3

# Streamlit & UI
streamlit>=1.30.0
streamlit-aggrid>=0.3.4
streamlit-option-menu>=0.3.6
streamlit-extras>=0.3.6

# Utils
pyyaml>=6.0
python-dotenv>=1.0.0
Pillow>=10.0.0
openpyxl>=3.1.2
tqdm>=4.66.0

# Development
pytest>=7.4.0
black>=23.0.0
flake8>=6.1.0
```

#### `config.yaml`
```yaml
# Configuraci√≥n General
app:
  name: "An√°lisis de Programas de Estudio"
  version: "1.0.0"
  debug: false

# Paths
paths:
  raw_pdfs: "data/raw"
  processed: "data/processed"
  cache: "data/cache"
  taxonomia: "data/taxonomia/habilidades.json"
  stopwords: "data/taxonomia/stopwords_custom.txt"
  outputs: "outputs"

# NLP Configuration
nlp:
  spacy_model: "es_core_news_lg"
  min_word_length: 3
  max_word_length: 30
  remove_numbers: true
  lemmatize: true
  pos_tags_keep: ["NOUN", "VERB", "ADJ"]
  
# Stopwords adicionales (acad√©micas)
stopwords_custom:
  - "universidad"
  - "asignatura"
  - "cr√©ditos"
  - "ects"
  - "semestre"
  - "curso"
  - "requisito"
  - "p√°gina"

# An√°lisis de Frecuencias
frequency:
  top_n_terms: 50
  ngram_range: [1, 3]
  tfidf_max_features: 500
  min_df: 2
  max_df: 0.8

# Topic Modeling
topics:
  default_n_topics: 10
  min_topics: 3
  max_topics: 20
  lda_iterations: 100
  lda_passes: 10
  coherence_threshold: 0.4

# Skills Mapping
skills:
  min_confidence: 0.3
  weight_tfidf: 0.6
  weight_frequency: 0.4

# UI Configuration
ui:
  theme: "light"
  max_upload_size_mb: 50
  results_per_page: 20
  cache_ttl_hours: 24
  
# Export
export:
  default_format: "xlsx"
  include_charts: true
  chart_dpi: 300
```

#### `src/utils/schemas.py` - **CONTRATO DE DATOS**
```python
"""
Schemas de datos para garantizar consistencia entre m√≥dulos.
Estos son los contratos que todos los m√≥dulos deben respetar.
"""
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from datetime import datetime
import pandas as pd


@dataclass
class ExtractedText:
    """Texto extra√≠do de un PDF"""
    filename: str
    raw_text: str
    metadata: Dict[str, str]
    page_count: int
    extraction_date: datetime
    has_tables: bool = False
    tables: List[pd.DataFrame] = field(default_factory=list)
    
    def to_dict(self) -> dict:
        return {
            'filename': self.filename,
            'raw_text': self.raw_text,
            'metadata': self.metadata,
            'page_count': self.page_count,
            'extraction_date': self.extraction_date.isoformat(),
            'has_tables': self.has_tables,
            'table_count': len(self.tables)
        }


@dataclass
class ProcessedText:
    """Texto procesado y limpio"""
    filename: str
    clean_text: str
    tokens: List[str]
    lemmas: List[str]
    pos_tags: List[str]
    entities: List[Dict[str, str]]
    metadata: Dict[str, str]
    processing_date: datetime
    
    def to_dict(self) -> dict:
        return {
            'filename': self.filename,
            'clean_text': self.clean_text,
            'token_count': len(self.tokens),
            'unique_tokens': len(set(self.tokens)),
            'metadata': self.metadata,
            'processing_date': self.processing_date.isoformat()
        }


@dataclass
class FrequencyAnalysis:
    """Resultado de an√°lisis de frecuencias"""
    document_id: str
    term_frequencies: pd.DataFrame  # columns: [term, frequency, tfidf]
    ngrams: Dict[int, pd.DataFrame]  # {1: unigrams_df, 2: bigrams_df, 3: trigrams_df}
    top_terms: List[str]
    vocabulary_size: int
    analysis_date: datetime


@dataclass
class Topic:
    """Representaci√≥n de un topic"""
    topic_id: int
    keywords: List[str]
    weights: List[float]
    label: str  # Etiqueta manual
    coherence_score: float
    documents: List[str]  # IDs de documentos donde aparece


@dataclass
class TopicModelResult:
    """Resultado de topic modeling"""
    model_type: str  # 'LDA' o 'NMF'
    n_topics: int
    topics: List[Topic]
    document_topic_matrix: pd.DataFrame
    coherence_score: float
    perplexity: Optional[float] = None


@dataclass
class Skill:
    """Definici√≥n de una habilidad"""
    skill_id: str
    name: str
    keywords: List[str]
    synonyms: List[str]
    weight: float
    category: str


@dataclass
class SkillScore:
    """Score de una habilidad en un documento"""
    skill_id: str
    skill_name: str
    score: float  # 0-1
    confidence: float  # 0-1
    matched_terms: List[str]
    context_snippets: List[str]


@dataclass
class DocumentSkillProfile:
    """Perfil de habilidades de un documento"""
    document_id: str
    skill_scores: List[SkillScore]
    top_skills: List[str]
    skill_coverage: float  # % del texto mapeado a habilidades
    analysis_date: datetime
    
    def to_dataframe(self) -> pd.DataFrame:
        """Convierte a DataFrame para visualizaci√≥n"""
        return pd.DataFrame([
            {
                'skill': ss.skill_name,
                'score': ss.score,
                'confidence': ss.confidence,
                'matched_terms': ', '.join(ss.matched_terms[:3])
            }
            for ss in self.skill_scores
        ])
```

#### `tests/conftest.py` - **Fixtures compartidas**
```python
"""
Fixtures de pytest para datos mock.
Permite desarrollo paralelo sin PDFs reales.
"""
import pytest
from datetime import datetime
from src.utils.schemas import ExtractedText, ProcessedText
import pandas as pd


@pytest.fixture
def mock_extracted_text():
    """PDF extra√≠do simulado"""
    return ExtractedText(
        filename="Matematicas_I.pdf",
        raw_text="""
        MATEM√ÅTICAS I
        
        Objetivos:
        - Desarrollar pensamiento l√≥gico y anal√≠tico
        - Resolver problemas mediante algoritmos
        - Aplicar t√©cnicas de c√°lculo diferencial
        
        Contenidos:
        1. √Ålgebra lineal
        2. C√°lculo diferencial
        3. M√©todos num√©ricos
        
        Evaluaci√≥n:
        - Ex√°menes parciales (40%)
        - Proyecto final (30%)
        - Tareas (30%)
        """,
        metadata={
            'programa': 'Ingenier√≠a en Computaci√≥n',
            'facultad': 'Ciencias',
            'a√±o': '2024',
            'semestre': '1'
        },
        page_count=5,
        extraction_date=datetime.now(),
        has_tables=False
    )


@pytest.fixture
def mock_processed_text():
    """Texto procesado simulado"""
    return ProcessedText(
        filename="Matematicas_I.pdf",
        clean_text="matem√°ticas desarrollar pensamiento l√≥gico anal√≠tico resolver problemas algoritmos",
        tokens=['matem√°ticas', 'desarrollar', 'pensamiento', 'l√≥gico', 'anal√≠tico', 
                'resolver', 'problemas', 'algoritmos', 'aplicar', 'c√°lculo'],
        lemmas=['matem√°tica', 'desarrollar', 'pensamiento', 'l√≥gico', 'anal√≠tico',
                'resolver', 'problema', 'algoritmo', 'aplicar', 'c√°lculo'],
        pos_tags=['NOUN', 'VERB', 'NOUN', 'ADJ', 'ADJ', 
                  'VERB', 'NOUN', 'NOUN', 'VERB', 'NOUN'],
        entities=[],
        metadata={
            'programa': 'Ingenier√≠a en Computaci√≥n',
            'a√±o': '2024'
        },
        processing_date=datetime.now()
    )


@pytest.fixture
def mock_frequency_df():
    """DataFrame de frecuencias simulado"""
    return pd.DataFrame({
        'term': ['algoritmo', 'problema', 'c√°lculo', 'l√≥gico', 'anal√≠tico'],
        'frequency': [15, 12, 10, 8, 7],
        'tfidf': [0.85, 0.72, 0.68, 0.55, 0.51]
    })


@pytest.fixture
def mock_taxonomia():
    """Taxonom√≠a de habilidades simulada"""
    return {
        "pensamiento_critico": {
            "name": "Pensamiento Cr√≠tico",
            "keywords": ["an√°lisis", "evaluar", "criticar", "argumentar", "razonar"],
            "synonyms": ["anal√≠tico", "cr√≠tico", "evaluativo"],
            "weight": 1.0,
            "category": "cognitiva"
        },
        "programacion": {
            "name": "Programaci√≥n",
            "keywords": ["c√≥digo", "programar", "algoritmo", "software", "desarrollo"],
            "synonyms": ["codificar", "implementar", "desarrollar"],
            "weight": 1.0,
            "category": "tecnica"
        },
        "resolucion_problemas": {
            "name": "Resoluci√≥n de Problemas",
            "keywords": ["resolver", "problema", "soluci√≥n", "optimizar"],
            "synonyms": ["solucionar", "abordar"],
            "weight": 1.0,
            "category": "cognitiva"
        },
        "matematicas": {
            "name": "Matem√°ticas",
            "keywords": ["c√°lculo", "√°lgebra", "geometr√≠a", "estad√≠stica", "matem√°tica"],
            "synonyms": ["num√©rico", "cuantitativo"],
            "weight": 0.9,
            "category": "tecnica"
        }
    }
```

---

## FASE 1: Backend Core (Desarrollo en Paralelo) - 1 semana

### **TRACK A: Extracci√≥n** (Desarrollador A / Claude Code sesi√≥n 1)

#### **Prioridad 1: `src/extraction/pdf_extractor.py`**
```python
"""
Extractor de texto de PDFs usando pdfplumber.

CONTRATO:
- Input: archivo PDF (bytes o path)
- Output: ExtractedText object
- Maneja: PDFs con texto, im√°genes, tablas
"""
import pdfplumber
from src.utils.schemas import ExtractedText
from datetime import datetime
from typing import Union, BinaryIO
import logging

logger = logging.getLogger(__name__)


class PDFExtractor:
    """Extrae texto y metadatos de PDFs"""
    
    def __init__(self, extract_tables: bool = True):
        self.extract_tables = extract_tables
    
    def extract(self, pdf_source: Union[str, BinaryIO], 
                metadata: dict = None) -> ExtractedText:
        """
        Extrae texto completo de un PDF.
        
        Args:
            pdf_source: Path al PDF o file-like object
            metadata: Metadata adicional (programa, facultad, etc.)
        
        Returns:
            ExtractedText object con texto y metadata
        
        Raises:
            ValueError: Si el PDF est√° corrupto o vac√≠o
        """
        try:
            with pdfplumber.open(pdf_source) as pdf:
                # Extraer texto de todas las p√°ginas
                full_text = []
                tables = []
                
                for page in pdf.pages:
                    # Texto
                    page_text = page.extract_text()
                    if page_text:
                        full_text.append(page_text)
                    
                    # Tablas (opcional)
                    if self.extract_tables:
                        page_tables = page.extract_tables()
                        if page_tables:
                            # Convertir a DataFrame
                            import pandas as pd
                            for table in page_tables:
                                if table:  # Verificar no vac√≠a
                                    df = pd.DataFrame(table[1:], columns=table[0])
                                    tables.append(df)
                
                # Validar contenido
                combined_text = '\n'.join(full_text)
                if not combined_text.strip():
                    raise ValueError("PDF vac√≠o o sin texto extra√≠ble")
                
                # Extraer metadata del PDF
                pdf_metadata = pdf.metadata or {}
                
                # Combinar con metadata provista
                final_metadata = {
                    'pdf_title': pdf_metadata.get('Title', ''),
                    'pdf_author': pdf_metadata.get('Author', ''),
                    'pdf_created': str(pdf_metadata.get('CreationDate', '')),
                    **(metadata or {})
                }
                
                # Inferir filename
                filename = getattr(pdf_source, 'name', 'unknown.pdf')
                if hasattr(filename, 'split'):
                    filename = filename.split('/')[-1]
                
                return ExtractedText(
                    filename=filename,
                    raw_text=combined_text,
                    metadata=final_metadata,
                    page_count=len(pdf.pages),
                    extraction_date=datetime.now(),
                    has_tables=len(tables) > 0,
                    tables=tables
                )
        
        except Exception as e:
            logger.error(f"Error extrayendo PDF: {str(e)}")
            raise ValueError(f"No se pudo extraer texto del PDF: {str(e)}")
    
    def extract_metadata_only(self, pdf_source: Union[str, BinaryIO]) -> dict:
        """Extrae solo metadata sin procesar texto completo"""
        with pdfplumber.open(pdf_source) as pdf:
            return {
                'page_count': len(pdf.pages),
                'metadata': pdf.metadata,
                'has_text': bool(pdf.pages[0].extract_text()) if pdf.pages else False
            }
```

#### **Test: `tests/test_extraction.py`**
```python
import pytest
from src.extraction.pdf_extractor import PDFExtractor
from src.utils.schemas import ExtractedText


def test_extractor_with_mock(tmp_path):
    """Test con PDF mock creado en memoria"""
    from reportlab.pdfgen import canvas
    
    # Crear PDF simple
    pdf_path = tmp_path / "test.pdf"
    c = canvas.Canvas(str(pdf_path))
    c.drawString(100, 750, "Test PDF")
    c.drawString(100, 700, "Matem√°ticas y algoritmos")
    c.save()
    
    # Extraer
    extractor = PDFExtractor()
    result = extractor.extract(str(pdf_path))
    
    # Validar contrato
    assert isinstance(result, ExtractedText)
    assert result.filename == "test.pdf"
    assert "Test PDF" in result.raw_text
    assert result.page_count == 1
    assert isinstance(result.extraction_date, datetime)


def test_extractor_handles_empty_pdf(tmp_path):
    """Test que maneja PDFs vac√≠os correctamente"""
    # Crear PDF vac√≠o
    pdf_path = tmp_path / "empty.pdf"
    c = canvas.Canvas(str(pdf_path))
    c.save()
    
    extractor = PDFExtractor()
    
    with pytest.raises(ValueError, match="vac√≠o"):
        extractor.extract(str(pdf_path))
```

---

#### **Prioridad 2: `src/extraction/preprocessor.py`**
```python
"""
Preprocessor de texto extra√≠do de PDFs.

CONTRATO:
- Input: ExtractedText object
- Output: ProcessedText object
- Limpia, tokeniza, lematiza, etiqueta POS
"""
import spacy
import re
from typing import List, Set
from src.utils.schemas import ExtractedText, ProcessedText
from src.utils.config import load_config
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class TextPreprocessor:
    """Limpia y procesa texto acad√©mico en espa√±ol"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = load_config(config_path)
        
        # Cargar modelo spaCy
        model_name = self.config['nlp']['spacy_model']
        try:
            self.nlp = spacy.load(model_name)
        except OSError:
            logger.warning(f"Modelo {model_name} no encontrado. Descargando...")
            import subprocess
            subprocess.run(["python", "-m", "spacy", "download", model_name])
            self.nlp = spacy.load(model_name)
        
        # Stopwords custom
        self.custom_stopwords = self._load_stopwords()
        
        # Configuraci√≥n
        self.min_length = self.config['nlp']['min_word_length']
        self.max_length = self.config['nlp']['max_word_length']
        self.pos_keep = set(self.config['nlp']['pos_tags_keep'])
    
    def _load_stopwords(self) -> Set[str]:
        """Carga stopwords est√°ndar + custom"""
        # spaCy stopwords
        stopwords = set(self.nlp.Defaults.stop_words)
        
        # Agregar custom
        custom = self.config.get('stopwords_custom', [])
        stopwords.update(custom)
        
        # Cargar de archivo si existe
        stopwords_file = self.config['paths'].get('stopwords')
        if stopwords_file:
            try:
                with open(stopwords_file, 'r', encoding='utf-8') as f:
                    file_stopwords = [line.strip() for line in f]
                    stopwords.update(file_stopwords)
            except FileNotFoundError:
                logger.warning(f"Archivo stopwords no encontrado: {stopwords_file}")
        
        return stopwords
    
    def process(self, extracted: ExtractedText) -> ProcessedText:
        """
        Procesa texto extra√≠do.
        
        Pipeline:
        1. Limpieza b√°sica (lowercase, remover especiales)
        2. Tokenizaci√≥n con spaCy
        3. Filtrado (stopwords, longitud, POS)
        4. Lematizaci√≥n
        5. Extracci√≥n de entidades
        
        Args:
            extracted: ExtractedText object
        
        Returns:
            ProcessedText object
        """
        # 1. Limpieza b√°sica
        clean = self._clean_text(extracted.raw_text)
        
        # 2. Procesar con spaCy
        doc = self.nlp(clean)
        
        # 3. Filtrar tokens
        filtered_tokens = []
        lemmas = []
        pos_tags = []
        
        for token in doc:
            # Filtros
            if token.is_space or token.is_punct:
                continue
            if token.text.lower() in self.custom_stopwords:
                continue
            if len(token.text) < self.min_length or len(token.text) > self.max_length:
                continue
            if self.pos_keep and token.pos_ not in self.pos_keep:
                continue
            
            filtered_tokens.append(token.text.lower())
            lemmas.append(token.lemma_.lower())
            pos_tags.append(token.pos_)
        
        # 4. Extraer entidades
        entities = [
            {
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            }
            for ent in doc.ents
        ]
        
        return ProcessedText(
            filename=extracted.filename,
            clean_text=' '.join(filtered_tokens),
            tokens=filtered_tokens,
            lemmas=lemmas,
            pos_tags=pos_tags,
            entities=entities,
            metadata=extracted.metadata,
            processing_date=datetime.now()
        )
    
    def _clean_text(self, text: str) -> str:
        """Limpieza b√°sica de texto"""
        # Lowercase
        text = text.lower()
        
        # Remover URLs
        text = re.sub(r'http\S+|www\S+', '', text)
        
        # Remover emails
        text = re.sub(r'\S+@\S+', '', text)
        
        # Normalizar espacios
        text = re.sub(r'\s+', ' ', text)
        
        # Remover n√∫meros si configurado
        if self.config['nlp'].get('remove_numbers', False):
            text = re.sub(r'\d+', '', text)
        
        return text.strip()
    
    def add_stopword(self, word: str):
        """Agrega stopword custom"""
        self.custom_stopwords.add(word.lower())
    
    def remove_stopword(self, word: str):
        """Remueve stopword custom"""
        self.custom_stopwords.discard(word.lower())
```

---

### **TRACK B: An√°lisis de Frecuencias** (Desarrollador B / Claude Code sesi√≥n 2)

#### **`src/analysis/frequency.py`**
```python
"""
An√°lisis de frecuencias: TF-IDF, n-grams, colocaciones.

CONTRATO:
- Input: ProcessedText o List[ProcessedText]
- Output: FrequencyAnalysis object
"""
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS  # Usaremos custom
import pandas as pd
import numpy as np
from typing import List, Dict
from src.utils.schemas import ProcessedText, FrequencyAnalysis
from src.utils.config import load_config
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class FrequencyAnalyzer:
    """Analiza frecuencias de t√©rminos en documentos"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = load_config(config_path)
        self.freq_config = self.config['frequency']
    
    def analyze_single(self, processed: ProcessedText) -> FrequencyAnalysis:
        """Analiza un solo documento"""
        return self.analyze_multiple([processed])[0]
    
    def analyze_multiple(self, documents: List[ProcessedText]) -> List[FrequencyAnalysis]:
        """
        Analiza m√∫ltiples documentos con TF-IDF.
        
        Args:
            documents: Lista de ProcessedText objects
        
        Returns:
            Lista de FrequencyAnalysis objects (uno por documento)
        """
        # Preparar corpus
        corpus = [doc.clean_text for doc in documents]
        doc_ids = [doc.filename for doc in documents]
        
        # TF-IDF
        tfidf_vectorizer = TfidfVectorizer(
            max_features=self.freq_config['tfidf_max_features'],
            min_df=self.freq_config['min_df'],
            max_df=self.freq_config['max_df'],
            ngram_range=(1, 1)  # Solo unigrams para TF-IDF
        )
        
        tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)
        feature_names = tfidf_vectorizer.get_feature_names_out()
        
        # Frecuencias simples
        count_vectorizer = CountVectorizer(
            max_features=self.freq_config['tfidf_max_features']
        )
        count_matrix = count_vectorizer.fit_transform(corpus)
        
        # Crear FrequencyAnalysis para cada documento
        results = []
        
        for idx, doc in enumerate(documents):
            # Extraer TF-IDF para este documento
            doc_tfidf = tfidf_matrix[idx].toarray().flatten()
            doc_counts = count_matrix[idx].toarray().flatten()
            
            # DataFrame de t√©rminos
            term_df = pd.DataFrame({
                'term': feature_names,
                'frequency': doc_counts,
                'tfidf': doc_tfidf
            })
            
            # Ordenar por TF-IDF
            term_df = term_df.sort_values('tfidf', ascending=False)
            
            # Top t√©rminos
            top_terms = term_df.nlargest(
                self.freq_config['top_n_terms'], 
                'tfidf'
            )['term'].tolist()
            
            # N-grams
            ngrams = self._extract_ngrams(doc.clean_text)
            
            results.append(FrequencyAnalysis(
                document_id=doc.filename,
                term_frequencies=term_df,
                ngrams=ngrams,
                top_terms=top_terms,
                vocabulary_size=len([f for f in doc_counts if f > 0]),
                analysis_date=datetime.now()
            ))
        
        return results
    
    def _extract_ngrams(self, text: str) -> Dict[int, pd.DataFrame]:
        """Extrae n-grams (1, 2, 3)"""
        ngram_results = {}
        
        min_n, max_n = self.freq_config['ngram_range']
        
        for n in range(min_n, max_n + 1):
            vectorizer = CountVectorizer(
                ngram_range=(n, n),
                max_features=50  # Top 50 por cada n
            )
            
            try:
                matrix = vectorizer.fit_transform([text])
                features = vectorizer.get_feature_names_out()
                counts = matrix.toarray().flatten()
                
                df = pd.DataFrame({
                    'ngram': features,
                    'frequency': counts
                }).sort_values('frequency', ascending=False)
                
                ngram_results[n] = df
            except ValueError:
                # No hay suficiente texto para este n-gram
                ngram_results[n] = pd.DataFrame(columns=['ngram', 'frequency'])
        
        return ngram_results
    
    def compare_documents(self, analyses: List[FrequencyAnalysis]) -> pd.DataFrame:
        """
        Compara t√©rminos entre documentos.
        
        Returns:
            DataFrame con t√©rminos en filas, documentos en columnas
        """
        # Obtener todos los t√©rminos √∫nicos
        all_terms = set()
        for analysis in analyses:
            all_terms.update(analysis.term_frequencies['term'])
        
        # Crear matriz documento-t√©rmino
        data = {}
        for analysis in analyses:
            term_dict = dict(zip(
                analysis.term_frequencies['term'],
                analysis.term_frequencies['tfidf']
            ))
            data[analysis.document_id] = [
                term_dict.get(term, 0.0) for term in all_terms
            ]
        
        return pd.DataFrame(data, index=list(all_terms))
    
    def get_cooccurrences(self, processed: ProcessedText, 
                          window_size: int = 5) -> pd.DataFrame:
        """
        Calcula co-ocurrencias de t√©rminos (palabras que aparecen juntas).
        
        Args:
            processed: ProcessedText object
            window_size: Ventana para considerar co-ocurrencia
        
        Returns:
            DataFrame con pares de t√©rminos y sus frecuencias
        """
        from collections import defaultdict
        
        tokens = processed.tokens
        cooccur = defaultdict(int)
        
        for i, token1 in enumerate(tokens):
            # Ventana de contexto
            start = max(0, i - window_size)
            end = min(len(tokens), i + window_size + 1)
            
            for j in range(start, end):
                if i != j:
                    token2 = tokens[j]
                    # Ordenar alfab√©ticamente para evitar duplicados
                    pair = tuple(sorted([token1, token2]))
                    cooccur[pair] += 1
        
        # Convertir a DataFrame
        df = pd.DataFrame([
            {'term1': pair[0], 'term2': pair[1], 'frequency': count}
            for pair, count in cooccur.items()
        ]).sort_values('frequency', ascending=False)
        
        return df
```

---

### **TRACK C: Topic Modeling** (Desarrollador C / Claude Code sesi√≥n 3)

#### **`src/analysis/topics.py`**
```python
"""
Topic Modeling con LDA y NMF.

CONTRATO:
- Input: List[ProcessedText]
- Output: TopicModelResult
"""
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import pandas as pd
import numpy as np
from typing import List, Literal
from src.utils.schemas import ProcessedText, Topic, TopicModelResult
from src.utils.config import load_config
import logging

logger = logging.getLogger(__name__)


class TopicModeler:
    """Descubre topics autom√°ticamente en corpus"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = load_config(config_path)
        self.topic_config = self.config['topics']
    
    def fit(self, 
            documents: List[ProcessedText],
            n_topics: int = None,
            method: Literal['lda', 'nmf'] = 'lda') -> TopicModelResult:
        """
        Entrena modelo de topics.
        
        Args:
            documents: Lista de documentos procesados
            n_topics: N√∫mero de topics (None = usar config)
            method: 'lda' o 'nmf'
        
        Returns:
            TopicModelResult con topics y asignaciones
        """
        if n_topics is None:
            n_topics = self.topic_config['default_n_topics']
        
        # Validar rango
        n_topics = max(
            self.topic_config['min_topics'],
            min(n_topics, self.topic_config['max_topics'])
        )
        
        # Preparar corpus
        corpus = [doc.clean_text for doc in documents]
        doc_ids = [doc.filename for doc in documents]
        
        if method == 'lda':
            return self._fit_lda(corpus, doc_ids, n_topics)
        elif method == 'nmf':
            return self._fit_nmf(corpus, doc_ids, n_topics)
        else:
            raise ValueError(f"M√©todo no soportado: {method}")
    
    def _fit_lda(self, corpus: List[str], doc_ids: List[str], 
                 n_topics: int) -> TopicModelResult:
        """Latent Dirichlet Allocation"""
        # Vectorizaci√≥n con CountVectorizer (LDA necesita frecuencias)
        vectorizer = CountVectorizer(
            max_features=500,
            min_df=2,
            max_df=0.8
        )
        doc_term_matrix = vectorizer.fit_transform(corpus)
        feature_names = vectorizer.get_feature_names_out()
        
        # LDA
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            max_iter=self.topic_config['lda_iterations'],
            learning_method='online',
            random_state=42,
            n_jobs=-1
        )
        
        doc_topic_matrix = lda.fit_transform(doc_term_matrix)
        
        # Extraer topics
        topics = []
        for topic_idx, topic_dist in enumerate(lda.components_):
            # Top keywords
            top_indices = topic_dist.argsort()[-10:][::-1]
            keywords = [feature_names[i] for i in top_indices]
            weights = [topic_dist[i] for i in top_indices]
            
            # Documentos donde aparece fuerte
            docs_with_topic = [
                doc_ids[i] for i in range(len(doc_ids))
                if doc_topic_matrix[i, topic_idx] > 0.3
            ]
            
            topics.append(Topic(
                topic_id=topic_idx,
                keywords=keywords,
                weights=weights,
                label=f"Topic {topic_idx}",  # Auto-etiquetar despu√©s
                coherence_score=0.0,  # Calcular despu√©s
                documents=docs_with_topic
            ))
        
        # Matriz documento-topic
        doc_topic_df = pd.DataFrame(
            doc_topic_matrix,
            index=doc_ids,
            columns=[f"topic_{i}" for i in range(n_topics)]
        )
        
        # Calcular coherence (simplificado)
        coherence = self._calculate_coherence_simple(lda, doc_term_matrix, feature_names)
        
        return TopicModelResult(
            model_type='LDA',
            n_topics=n_topics,
            topics=topics,
            document_topic_matrix=doc_topic_df,
            coherence_score=coherence,
            perplexity=lda.perplexity(doc_term_matrix)
        )
    
    def _fit_nmf(self, corpus: List[str], doc_ids: List[str],
                 n_topics: int) -> TopicModelResult:
        """Non-negative Matrix Factorization"""
        # Vectorizaci√≥n con TF-IDF (NMF funciona mejor con TF-IDF)
        vectorizer = TfidfVectorizer(
            max_features=500,
            min_df=2,
            max_df=0.8
        )
        doc_term_matrix = vectorizer.fit_transform(corpus)
        feature_names = vectorizer.get_feature_names_out()
        
        # NMF
        nmf = NMF(
            n_components=n_topics,
            random_state=42,
            max_iter=200,
            init='nndsvda'  # Mejor inicializaci√≥n
        )
        
        doc_topic_matrix = nmf.fit_transform(doc_term_matrix)
        
        # Extraer topics (similar a LDA)
        topics = []
        for topic_idx, topic_dist in enumerate(nmf.components_):
            top_indices = topic_dist.argsort()[-10:][::-1]
            keywords = [feature_names[i] for i in top_indices]
            weights = [topic_dist[i] for i in top_indices]
            
            docs_with_topic = [
                doc_ids[i] for i in range(len(doc_ids))
                if doc_topic_matrix[i, topic_idx] > 0.3
            ]
            
            topics.append(Topic(
                topic_id=topic_idx,
                keywords=keywords,
                weights=weights,
                label=f"Topic {topic_idx}",
                coherence_score=0.0,
                documents=docs_with_topic
            ))
        
        doc_topic_df = pd.DataFrame(
            doc_topic_matrix,
            index=doc_ids,
            columns=[f"topic_{i}" for i in range(n_topics)]
        )
        
        coherence = self._calculate_coherence_simple(nmf, doc_term_matrix, feature_names)
        
        return TopicModelResult(
            model_type='NMF',
            n_topics=n_topics,
            topics=topics,
            document_topic_matrix=doc_topic_df,
            coherence_score=coherence,
            perplexity=None  # NMF no tiene perplexity
        )
    
    def _calculate_coherence_simple(self, model, doc_term_matrix, 
                                    feature_names) -> float:
        """
        Coherence score simplificado (UMass).
        Para coherence real, usar gensim.
        """
        # Esta es una versi√≥n simplificada
        # TODO: Implementar coherence real con gensim si es necesario
        
        # Por ahora, devolver un score basado en la distribuci√≥n
        scores = []
        for topic_dist in model.components_:
            # Entrop√≠a normalizada del topic
            topic_dist_norm = topic_dist / topic_dist.sum()
            entropy = -np.sum(topic_dist_norm * np.log(topic_dist_norm + 1e-10))
            scores.append(entropy)
        
        return float(np.mean(scores))
    
    def auto_label_topics(self, result: TopicModelResult) -> TopicModelResult:
        """
        Etiqueta topics autom√°ticamente basado en keywords.
        Usa heur√≠sticas simples.
        """
        for topic in result.topics:
            # Top 3 keywords
            top_3 = ' + '.join(topic.keywords[:3])
            topic.label = top_3.title()
        
        return result
    
    def find_optimal_topics(self, documents: List[ProcessedText],
                            min_topics: int = None,
                            max_topics: int = None,
                            method: str = 'lda') -> pd.DataFrame:
        """
        Encuentra n√∫mero √≥ptimo de topics probando m√∫ltiples valores.
        
        Returns:
            DataFrame con n_topics, coherence_score, perplexity (si LDA)
        """
        if min_topics is None:
            min_topics = self.topic_config['min_topics']
        if max_topics is None:
            max_topics = self.topic_config['max_topics']
        
        results = []
        
        for n in range(min_topics, max_topics + 1):
            logger.info(f"Probando {n} topics...")
            result = self.fit(documents, n_topics=n, method=method)
            
            results.append({
                'n_topics': n,
                'coherence': result.coherence_score,
                'perplexity': result.perplexity
            })
        
        return pd.DataFrame(results)
```
## FASE 1: Backend Core (Continuaci√≥n)

### **TRACK D: Skills Mapping** (Desarrollador D / Claude Code sesi√≥n 4)

#### **`src/analysis/skills_mapper.py`**
```python
"""
Mapeo de t√©rminos a habilidades usando taxonom√≠a definida.

CONTRATO:
- Input: FrequencyAnalysis + taxonom√≠a
- Output: DocumentSkillProfile
"""
import pandas as pd
import numpy as np
from typing import List, Dict
import json
from src.utils.schemas import (
    FrequencyAnalysis, 
    DocumentSkillProfile, 
    SkillScore,
    Skill
)
from src.utils.config import load_config
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


class SkillsMapper:
    """Mapea t√©rminos extra√≠dos a habilidades definidas"""
    
    def __init__(self, config_path: str = "config.yaml"):
        self.config = load_config(config_path)
        self.skills_config = self.config['skills']
        
        # Cargar taxonom√≠a
        self.taxonomia = self._load_taxonomia()
        self.skills = self._parse_taxonomia()
    
    def _load_taxonomia(self) -> dict:
        """Carga taxonom√≠a de habilidades desde JSON"""
        taxonomia_path = self.config['paths']['taxonomia']
        
        try:
            with open(taxonomia_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"Taxonom√≠a no encontrada en {taxonomia_path}")
            return self._get_default_taxonomia()
    
    def _get_default_taxonomia(self) -> dict:
        """Taxonom√≠a por defecto si no existe archivo"""
        return {
            "pensamiento_critico": {
                "name": "Pensamiento Cr√≠tico",
                "keywords": ["an√°lisis", "evaluar", "criticar", "argumentar", "razonar"],
                "synonyms": ["anal√≠tico", "cr√≠tico", "evaluativo"],
                "weight": 1.0,
                "category": "cognitiva"
            },
            "programacion": {
                "name": "Programaci√≥n",
                "keywords": ["c√≥digo", "programar", "algoritmo", "software", "desarrollo"],
                "synonyms": ["codificar", "implementar", "desarrollar"],
                "weight": 1.0,
                "category": "tecnica"
            },
            "resolucion_problemas": {
                "name": "Resoluci√≥n de Problemas",
                "keywords": ["resolver", "problema", "soluci√≥n", "optimizar"],
                "synonyms": ["solucionar", "abordar"],
                "weight": 1.0,
                "category": "cognitiva"
            }
        }
    
    def _parse_taxonomia(self) -> List[Skill]:
        """Convierte taxonom√≠a dict a objetos Skill"""
        skills = []
        for skill_id, data in self.taxonomia.items():
            skills.append(Skill(
                skill_id=skill_id,
                name=data['name'],
                keywords=data['keywords'],
                synonyms=data.get('synonyms', []),
                weight=data.get('weight', 1.0),
                category=data.get('category', 'general')
            ))
        return skills
    
    def map_document(self, frequency_analysis: FrequencyAnalysis,
                     processed_text: 'ProcessedText' = None) -> DocumentSkillProfile:
        """
        Mapea un documento a perfil de habilidades.
        
        Args:
            frequency_analysis: An√°lisis de frecuencias del documento
            processed_text: Texto procesado (opcional, para snippets)
        
        Returns:
            DocumentSkillProfile con scores de habilidades
        """
        skill_scores = []
        
        for skill in self.skills:
            score_result = self._calculate_skill_score(
                skill, 
                frequency_analysis,
                processed_text
            )
            skill_scores.append(score_result)
        
        # Ordenar por score
        skill_scores.sort(key=lambda x: x.score, reverse=True)
        
        # Top skills (score > min_confidence)
        min_conf = self.skills_config['min_confidence']
        top_skills = [
            ss.skill_name for ss in skill_scores 
            if ss.score > min_conf
        ]
        
        # Calcular coverage (% de t√©rminos mapeados)
        total_terms = len(frequency_analysis.term_frequencies)
        mapped_terms = sum(len(ss.matched_terms) for ss in skill_scores)
        coverage = min(mapped_terms / total_terms, 1.0) if total_terms > 0 else 0.0
        
        return DocumentSkillProfile(
            document_id=frequency_analysis.document_id,
            skill_scores=skill_scores,
            top_skills=top_skills[:10],  # Top 10
            skill_coverage=coverage,
            analysis_date=datetime.now()
        )
    
    def _calculate_skill_score(self, skill: Skill, 
                               freq_analysis: FrequencyAnalysis,
                               processed_text: 'ProcessedText' = None) -> SkillScore:
        """
        Calcula score de una habilidad para un documento.
        
        Score = weighted_avg(tfidf_score, frequency_score)
        """
        term_df = freq_analysis.term_frequencies
        
        # Todos los t√©rminos relacionados con la skill
        all_keywords = set(skill.keywords + skill.synonyms)
        
        # Encontrar matches
        matched_terms = []
        tfidf_sum = 0.0
        freq_sum = 0
        
        for keyword in all_keywords:
            # Buscar keyword en t√©rminos (match exacto o substring)
            matches = term_df[term_df['term'].str.contains(keyword, case=False, na=False)]
            
            if not matches.empty:
                matched_terms.extend(matches['term'].tolist())
                tfidf_sum += matches['tfidf'].sum()
                freq_sum += matches['frequency'].sum()
        
        matched_terms = list(set(matched_terms))  # √önicos
        
        # Normalizar scores
        max_tfidf = term_df['tfidf'].max() if not term_df.empty else 1.0
        max_freq = term_df['frequency'].max() if not term_df.empty else 1.0
        
        tfidf_score = (tfidf_sum / max_tfidf) if max_tfidf > 0 else 0.0
        freq_score = (freq_sum / max_freq) if max_freq > 0 else 0.0
        
        # Score combinado
        w_tfidf = self.skills_config['weight_tfidf']
        w_freq = self.skills_config['weight_frequency']
        
        final_score = (w_tfidf * tfidf_score + w_freq * freq_score) * skill.weight
        final_score = min(final_score, 1.0)  # Cap at 1.0
        
        # Confianza basada en # de matches
        confidence = min(len(matched_terms) / len(skill.keywords), 1.0)
        
        # Extraer snippets de contexto
        snippets = []
        if processed_text and matched_terms:
            snippets = self._extract_snippets(
                processed_text.clean_text, 
                matched_terms[:3]  # Primeros 3
            )
        
        return SkillScore(
            skill_id=skill.skill_id,
            skill_name=skill.name,
            score=final_score,
            confidence=confidence,
            matched_terms=matched_terms,
            context_snippets=snippets
        )
    
    def _extract_snippets(self, text: str, terms: List[str], 
                         window: int = 50) -> List[str]:
        """Extrae fragmentos de texto donde aparecen t√©rminos"""
        snippets = []
        text_lower = text.lower()
        
        for term in terms:
            pos = text_lower.find(term.lower())
            if pos != -1:
                start = max(0, pos - window)
                end = min(len(text), pos + len(term) + window)
                snippet = "..." + text[start:end] + "..."
                snippets.append(snippet)
        
        return snippets[:3]  # Max 3 snippets
    
    def create_skills_matrix(self, profiles: List[DocumentSkillProfile]) -> pd.DataFrame:
        """
        Crea matriz documento √ó habilidad.
        
        Args:
            profiles: Lista de perfiles de documentos
        
        Returns:
            DataFrame con documentos en filas, habilidades en columnas
        """
        # Todas las skills √∫nicas
        all_skills = set()
        for profile in profiles:
            all_skills.update([ss.skill_name for ss in profile.skill_scores])
        
        # Matriz
        data = {}
        for profile in profiles:
            skill_dict = {
                ss.skill_name: ss.score 
                for ss in profile.skill_scores
            }
            data[profile.document_id] = [
                skill_dict.get(skill, 0.0) for skill in all_skills
            ]
        
        df = pd.DataFrame(data, index=list(all_skills)).T
        return df
    
    def compare_profiles(self, profile1: DocumentSkillProfile,
                        profile2: DocumentSkillProfile) -> pd.DataFrame:
        """Compara dos perfiles de habilidades"""
        # Crear DataFrame comparativo
        skills = set(
            [ss.skill_name for ss in profile1.skill_scores] +
            [ss.skill_name for ss in profile2.skill_scores]
        )
        
        score1 = {ss.skill_name: ss.score for ss in profile1.skill_scores}
        score2 = {ss.skill_name: ss.score for ss in profile2.skill_scores}
        
        comparison = pd.DataFrame({
            profile1.document_id: [score1.get(s, 0.0) for s in skills],
            profile2.document_id: [score2.get(s, 0.0) for s in skills],
        }, index=list(skills))
        
        comparison['difference'] = abs(
            comparison.iloc[:, 0] - comparison.iloc[:, 1]
        )
        
        return comparison.sort_values('difference', ascending=False)
    
    def update_taxonomia(self, new_taxonomia: dict):
        """Actualiza taxonom√≠a y recarga skills"""
        self.taxonomia = new_taxonomia
        self.skills = self._parse_taxonomia()
        
        # Guardar a archivo
        taxonomia_path = self.config['paths']['taxonomia']
        with open(taxonomia_path, 'w', encoding='utf-8') as f:
            json.dump(new_taxonomia, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Taxonom√≠a actualizada con {len(self.skills)} habilidades")
```

---

## FASE 2: Visualizaciones (Desarrollo en Paralelo) - 3-4 d√≠as

### **TRACK E: Plotly Charts** (Claude Code sesi√≥n 5)

#### **`src/visualization/plotly_charts.py`**
```python
"""
Gr√°ficos interactivos con Plotly para Streamlit.

CONTRATO:
- Input: DataFrames de an√°lisis
- Output: plotly.graph_objects.Figure
"""
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
from typing import List, Optional


class PlotlyCharts:
    """Generador de gr√°ficos interactivos"""
    
    def __init__(self, theme: str = 'plotly_white'):
        self.theme = theme
        self.color_scale = 'Viridis'
    
    def plot_top_terms(self, df_freq: pd.DataFrame, 
                       top_n: int = 20,
                       title: str = "T√©rminos M√°s Frecuentes") -> go.Figure:
        """
        Gr√°fico de barras horizontal de top t√©rminos.
        
        Args:
            df_freq: DataFrame con columnas ['term', 'frequency', 'tfidf']
            top_n: N√∫mero de t√©rminos a mostrar
        """
        df_top = df_freq.nlargest(top_n, 'tfidf')
        
        fig = px.bar(
            df_top,
            x='tfidf',
            y='term',
            orientation='h',
            title=title,
            labels={'tfidf': 'TF-IDF Score', 'term': 'T√©rmino'},
            color='tfidf',
            color_continuous_scale=self.color_scale,
            hover_data=['frequency']
        )
        
        fig.update_layout(
            height=max(400, top_n * 25),  # Din√°mico seg√∫n n
            showlegend=False,
            yaxis={'categoryorder': 'total ascending'},
            template=self.theme,
            hovermode='closest'
        )
        
        return fig
    
    def plot_wordcloud_data(self, df_freq: pd.DataFrame) -> go.Figure:
        """
        Pseudo-wordcloud usando scatter plot.
        Para wordcloud real, usar wordcloud library separadamente.
        """
        # Este es un placeholder - wordcloud real va en wordclouds.py
        pass
    
    def plot_skills_heatmap(self, df_matrix: pd.DataFrame,
                           title: str = "Matriz de Habilidades") -> go.Figure:
        """
        Heatmap de documentos √ó habilidades.
        
        Args:
            df_matrix: DataFrame con documentos en filas, skills en columnas
        """
        fig = px.imshow(
            df_matrix,
            labels=dict(x="Habilidades", y="Documentos", color="Score"),
            x=df_matrix.columns,
            y=df_matrix.index,
            color_continuous_scale='RdYlGn',
            aspect="auto",
            title=title
        )
        
        fig.update_layout(
            height=max(400, len(df_matrix) * 30),
            template=self.theme,
            xaxis={'tickangle': -45}
        )
        
        # Annotations con valores
        annotations = []
        for i, row in enumerate(df_matrix.index):
            for j, col in enumerate(df_matrix.columns):
                val = df_matrix.loc[row, col]
                if val > 0.1:  # Solo mostrar valores significativos
                    annotations.append(
                        dict(
                            x=col,
                            y=row,
                            text=f"{val:.2f}",
                            showarrow=False,
                            font=dict(color='white' if val > 0.5 else 'black')
                        )
                    )
        
        fig.update_layout(annotations=annotations)
        
        return fig
    
    def plot_radar_skills(self, df_skills: pd.DataFrame,
                         programs: List[str],
                         title: str = "Comparaci√≥n de Habilidades") -> go.Figure:
        """
        Radar chart comparativo de habilidades entre programas.
        
        Args:
            df_skills: DataFrame con programas en filas, skills en columnas
            programs: Lista de programas a comparar (max 4)
        """
        fig = go.Figure()
        
        colors = ['#636EFA', '#EF553B', '#00CC96', '#AB63FA']
        
        for idx, programa in enumerate(programs[:4]):  # Max 4
            if programa in df_skills.index:
                values = df_skills.loc[programa].values.tolist()
                # Cerrar el pol√≠gono
                values += values[:1]
                categories = df_skills.columns.tolist() + [df_skills.columns[0]]
                
                fig.add_trace(go.Scatterpolar(
                    r=values,
                    theta=categories,
                    fill='toself',
                    name=programa,
                    line_color=colors[idx % len(colors)]
                ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1.0]
                )
            ),
            showlegend=True,
            title=title,
            template=self.theme,
            height=500
        )
        
        return fig
    
    def plot_ngrams_comparison(self, ngrams_dict: dict,
                              n: int = 2,
                              top: int = 15) -> go.Figure:
        """
        Compara n-grams entre documentos.
        
        Args:
            ngrams_dict: {doc_id: DataFrame de n-grams}
            n: Tipo de n-gram (1, 2, 3)
        """
        fig = go.Figure()
        
        for doc_id, df in ngrams_dict.items():
            df_n = df[df['ngram'].str.split().str.len() == n]
            df_top = df_n.nlargest(top, 'frequency')
            
            fig.add_trace(go.Bar(
                x=df_top['ngram'],
                y=df_top['frequency'],
                name=doc_id
            ))
        
        fig.update_layout(
            title=f"Top {top} {n}-grams por Documento",
            xaxis_title=f"{n}-gram",
            yaxis_title="Frecuencia",
            barmode='group',
            template=self.theme,
            height=500,
            xaxis={'tickangle': -45}
        )
        
        return fig
    
    def plot_cooccurrence_network(self, df_cooccur: pd.DataFrame,
                                  top_n: int = 30) -> go.Figure:
        """
        Network graph de co-ocurrencias.
        
        Args:
            df_cooccur: DataFrame con columnas ['term1', 'term2', 'frequency']
        """
        df_top = df_cooccur.nlargest(top_n, 'frequency')
        
        # Crear nodos √∫nicos
        nodes = list(set(df_top['term1'].tolist() + df_top['term2'].tolist()))
        node_indices = {node: idx for idx, node in enumerate(nodes)}
        
        # Crear edges
        edge_x = []
        edge_y = []
        edge_weights = []
        
        # Posiciones circulares (simplificado - usar networkx para layouts reales)
        angles = np.linspace(0, 2*np.pi, len(nodes), endpoint=False)
        node_x = np.cos(angles)
        node_y = np.sin(angles)
        
        for _, row in df_top.iterrows():
            idx1 = node_indices[row['term1']]
            idx2 = node_indices[row['term2']]
            
            edge_x.extend([node_x[idx1], node_x[idx2], None])
            edge_y.extend([node_y[idx1], node_y[idx2], None])
            edge_weights.append(row['frequency'])
        
        # Normalizar weights para opacidad
        max_weight = max(edge_weights)
        opacities = [w/max_weight for w in edge_weights]
        
        # Crear figura
        fig = go.Figure()
        
        # Edges
        fig.add_trace(go.Scatter(
            x=edge_x,
            y=edge_y,
            mode='lines',
            line=dict(color='lightgray', width=1),
            hoverinfo='none',
            showlegend=False
        ))
        
        # Nodes
        fig.add_trace(go.Scatter(
            x=node_x,
            y=node_y,
            mode='markers+text',
            marker=dict(
                size=20,
                color='lightblue',
                line=dict(color='darkblue', width=2)
            ),
            text=nodes,
            textposition='top center',
            hoverinfo='text',
            showlegend=False
        ))
        
        fig.update_layout(
            title="Red de Co-ocurrencias de T√©rminos",
            showlegend=False,
            template=self.theme,
            hovermode='closest',
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            height=600
        )
        
        return fig
    
    def plot_topic_distribution(self, doc_topic_df: pd.DataFrame,
                               topic_labels: List[str] = None) -> go.Figure:
        """
        Distribuci√≥n de topics en documentos.
        
        Args:
            doc_topic_df: DataFrame con docs en filas, topics en columnas
            topic_labels: Etiquetas de topics (opcional)
        """
        # Stacked bar chart
        if topic_labels:
            doc_topic_df.columns = topic_labels
        
        fig = go.Figure()
        
        for col in doc_topic_df.columns:
            fig.add_trace(go.Bar(
                x=doc_topic_df.index,
                y=doc_topic_df[col],
                name=col
            ))
        
        fig.update_layout(
            barmode='stack',
            title="Distribuci√≥n de Topics por Documento",
            xaxis_title="Documento",
            yaxis_title="Proporci√≥n",
            template=self.theme,
            height=500,
            xaxis={'tickangle': -45}
        )
        
        return fig
    
    def plot_skill_trends(self, df_temporal: pd.DataFrame,
                         skill_name: str) -> go.Figure:
        """
        Tendencia temporal de una habilidad.
        
        Args:
            df_temporal: DataFrame con columnas ['year', 'programa', 'score']
            skill_name: Nombre de la habilidad
        """
        fig = px.line(
            df_temporal,
            x='year',
            y='score',
            color='programa',
            markers=True,
            title=f"Evoluci√≥n de '{skill_name}' a trav√©s del Tiempo",
            labels={'score': 'Score', 'year': 'A√±o'}
        )
        
        fig.update_layout(
            template=self.theme,
            height=400,
            hovermode='x unified'
        )
        
        return fig
```

#### **`src/visualization/wordclouds.py`**
```python
"""
Generaci√≥n de WordClouds.
"""
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
from typing import Optional
import io


class WordCloudGenerator:
    """Genera wordclouds personalizadas"""
    
    def __init__(self, width: int = 800, height: int = 400):
        self.width = width
        self.height = height
    
    def generate(self, text: str, 
                mask_image: Optional[np.ndarray] = None,
                colormap: str = 'viridis',
                background_color: str = 'white') -> Image.Image:
        """
        Genera wordcloud.
        
        Args:
            text: Texto para wordcloud
            mask_image: Imagen para forma custom (opcional)
            colormap: Colormap de matplotlib
        
        Returns:
            PIL Image
        """
        wc = WordCloud(
            width=self.width,
            height=self.height,
            background_color=background_color,
            colormap=colormap,
            mask=mask_image,
            relative_scaling=0.5,
            min_font_size=10
        )
        
        wc.generate(text)
        
        # Convertir a PIL Image
        return wc.to_image()
    
    def generate_from_frequencies(self, freq_dict: dict,
                                 **kwargs) -> Image.Image:
        """Genera wordcloud desde diccionario de frecuencias"""
        wc = WordCloud(
            width=self.width,
            height=self.height,
            **kwargs
        )
        
        wc.generate_from_frequencies(freq_dict)
        return wc.to_image()
    
    def to_bytes(self, image: Image.Image, format: str = 'PNG') -> bytes:
        """Convierte PIL Image a bytes para Streamlit"""
        buf = io.BytesIO()
        image.save(buf, format=format)
        buf.seek(0)
        return buf.getvalue()
```

---

## FASE 3: Streamlit App (Desarrollo en Paralelo) - 1 semana

### **Setup Inicial de Streamlit**

#### **`.streamlit/config.toml`**
```toml
[theme]
primaryColor = "#1f77b4"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0f2f6"
textColor = "#262730"
font = "sans serif"

[server]
maxUploadSize = 50
enableXsrfProtection = true
enableCORS = false
```

#### **`app/utils/session_manager.py`** - **CR√çTICO PARA STREAMLIT**
```python
"""
Gesti√≥n centralizada de st.session_state.
"""
import streamlit as st
from typing import Any, Dict
from src.utils.config import load_config


def init_session_state():
    """Inicializa todas las variables de sesi√≥n necesarias"""
    
    # Config
    if 'config' not in st.session_state:
        st.session_state.config = load_config()
    
    # PDFs y procesamiento
    if 'uploaded_pdfs' not in st.session_state:
        st.session_state.uploaded_pdfs = {}  # {filename: file_bytes}
    
    if 'extracted_texts' not in st.session_state:
        st.session_state.extracted_texts = {}  # {filename: ExtractedText}
    
    if 'processed_texts' not in st.session_state:
        st.session_state.processed_texts = {}  # {filename: ProcessedText}
    
    # An√°lisis
    if 'frequency_analyses' not in st.session_state:
        st.session_state.frequency_analyses = {}  # {filename: FrequencyAnalysis}
    
    if 'topic_model' not in st.session_state:
        st.session_state.topic_model = None  # TopicModelResult
    
    if 'skill_profiles' not in st.session_state:
        st.session_state.skill_profiles = {}  # {filename: DocumentSkillProfile}
    
    # Taxonom√≠a
    if 'taxonomia' not in st.session_state:
        from src.analysis.skills_mapper import SkillsMapper
        mapper = SkillsMapper()
        st.session_state.taxonomia = mapper.taxonomia
    
    # UI State
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 'Home'
    
    if 'selected_documents' not in st.session_state:
        st.session_state.selected_documents = []
    
    if 'processing_status' not in st.session_state:
        st.session_state.processing_status = {}  # {filename: status_msg}


def get_state(key: str, default: Any = None) -> Any:
    """Get value from session state"""
    return st.session_state.get(key, default)


def set_state(key: str, value: Any):
    """Set value in session state"""
    st.session_state[key] = value


def clear_state(key: str):
    """Clear a session state key"""
    if key in st.session_state:
        del st.session_state[key]


def reset_analysis():
    """Reset all analysis results"""
    st.session_state.frequency_analyses = {}
    st.session_state.topic_model = None
    st.session_state.skill_profiles = {}
```

#### **`app/utils/cache_manager.py`**
```python
"""
Decoradores de cach√© para funciones pesadas.
"""
import streamlit as st
from functools import wraps
import hashlib
import pickle


@st.cache_data(ttl=3600, show_spinner="Extrayendo texto...")
def cached_pdf_extraction(pdf_bytes: bytes, filename: str):
    """Cache de extracci√≥n de PDFs"""
    from src.extraction.pdf_extractor import PDFExtractor
    
    extractor = PDFExtractor()
    # pdf_bytes viene de uploaded_file.read()
    import io
    pdf_file = io.BytesIO(pdf_bytes)
    
    return extractor.extract(pdf_file, metadata={'filename': filename})


@st.cache_data(ttl=3600, show_spinner="Procesando texto...")
def cached_text_processing(extracted_dict: dict):
    """Cache de procesamiento de texto"""
    from src.extraction.preprocessor import TextPreprocessor
    from src.utils.schemas import ExtractedText
    from datetime import datetime
    
    # Reconstruir ExtractedText desde dict
    extracted = ExtractedText(
        filename=extracted_dict['filename'],
        raw_text=extracted_dict['raw_text'],
        metadata=extracted_dict['metadata'],
        page_count=extracted_dict['page_count'],
        extraction_date=datetime.fromisoformat(extracted_dict['extraction_date']),
        has_tables=extracted_dict.get('has_tables', False),
        tables=[]
    )
    
    preprocessor = TextPreprocessor()
    return preprocessor.process(extracted)


@st.cache_resource
def load_spacy_model():
    """Carga modelo spaCy una sola vez"""
    import spacy
    return spacy.load("es_core_news_lg")


@st.cache_data(ttl=7200, show_spinner="Analizando frecuencias...")
def cached_frequency_analysis(processed_dict: dict):
    """Cache de an√°lisis de frecuencias"""
    from src.analysis.frequency import FrequencyAnalyzer
    from src.utils.schemas import ProcessedText
    from datetime import datetime
    
    # Reconstruir ProcessedText
    processed = ProcessedText(
        filename=processed_dict['filename'],
        clean_text=processed_dict['clean_text'],
        tokens=processed_dict['tokens'],
        lemmas=processed_dict['lemmas'],
        pos_tags=processed_dict['pos_tags'],
        entities=processed_dict['entities'],
        metadata=processed_dict['metadata'],
        processing_date=datetime.fromisoformat(processed_dict['processing_date'])
    )
    
    analyzer = FrequencyAnalyzer()
    return analyzer.analyze_single(processed)


@st.cache_data(ttl=7200, show_spinner="Generando modelo de topics...")
def cached_topic_modeling(processed_dicts: list, n_topics: int, method: str):
    """Cache de topic modeling"""
    from src.analysis.topics import TopicModeler
    from src.utils.schemas import ProcessedText
    from datetime import datetime
    
    # Reconstruir ProcessedTexts
    documents = []
    for pd in processed_dicts:
        doc = ProcessedText(
            filename=pd['filename'],
            clean_text=pd['clean_text'],
            tokens=pd['tokens'],
            lemmas=pd['lemmas'],
            pos_tags=pd['pos_tags'],
            entities=pd['entities'],
            metadata=pd['metadata'],
            processing_date=datetime.fromisoformat(pd['processing_date'])
        )
        documents.append(doc)
    
    modeler = TopicModeler()
    return modeler.fit(documents, n_topics=n_topics, method=method)
```

---

### **P√°gina Principal: `app/Home.py`**

```python
"""
P√°gina principal de la aplicaci√≥n Streamlit.
"""
import streamlit as st
from app.utils.session_manager import init_session_state
import sys
from pathlib import Path

# Agregar src al path
root_dir = Path(__file__).parent.parent
sys.path.append(str(root_dir))


# Configuraci√≥n de p√°gina
st.set_page_config(
    page_title="An√°lisis de Programas de Estudio",
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Inicializar session state
init_session_state()

# CSS Custom
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1.5rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .info-box {
        background-color: #e7f3ff;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #0066cc;
    }
</style>
""", unsafe_allow_html=True)

# Header
st.markdown('<div class="main-header">üéì An√°lisis de Programas de Estudio</div>', 
            unsafe_allow_html=True)

# Descripci√≥n
st.markdown("""
### Bienvenido al Sistema de An√°lisis Curricular

Esta aplicaci√≥n te permite analizar programas de estudio universitarios para identificar:
- **T√©rminos m√°s frecuentes** y relevantes
- **Topics tem√°ticos** autom√°ticos
- **Habilidades trabajadas** en cada programa
- **Comparativas** entre programas

#### üöÄ C√≥mo empezar:
1. Ve a **üìÅ Subir PDFs** para cargar tus documentos
2. Explora **üìä An√°lisis de Frecuencias** para ver t√©rminos principales
3. Descubre **üéØ Topics y Habilidades** identificados autom√°ticamente
4. Compara programas en **üìà Comparativa**
""")

# Estad√≠sticas actuales
st.markdown("---")
st.subheader("üìä Estado Actual del Sistema")

col1, col2, col3, col4 = st.columns(4)

with col1:
    n_pdfs = len(st.session_state.uploaded_pdfs)
    st.metric("PDFs Cargados", n_pdfs, delta=None)

with col2:
    n_processed = len(st.session_state.processed_texts)
    st.metric("Documentos Procesados", n_processed)

with col3:
    n_analyzed = len(st.session_state.frequency_analyses)
    st.metric("An√°lisis Realizados", n_analyzed)

with col4:
    has_topics = st.session_state.topic_model is not None
    st.metric("Modelo de Topics", "‚úÖ" if has_topics else "‚ùå")

# Informaci√≥n adicional
if n_pdfs == 0:
    st.info("üëà Comienza subiendo PDFs en la secci√≥n **üìÅ Subir PDFs** del men√∫ lateral")
else:
    st.success(f"‚úÖ Tienes {n_pdfs} PDFs listos para analizar")
    
    # Mostrar lista de documentos
    with st.expander("üìÑ Ver documentos cargados"):
        for filename in st.session_state.uploaded_pdfs.keys():
            status = "‚úÖ Procesado" if filename in st.session_state.processed_texts else "‚è≥ Pendiente"
            st.write(f"- {filename} - {status}")

# Quick Actions
st.markdown("---")
st.subheader("‚ö° Acciones R√°pidas")

col_a, col_b, col_c = st.columns(3)

with col_a:
    if st.button("üîÑ Recargar Configuraci√≥n", use_container_width=True):
        from src.utils.config import load_config
        st.session_state.config = load_config()
        st.success("Configuraci√≥n recargada")
        st.rerun()

with col_b:
    if st.button("üóëÔ∏è Limpiar Cach√©", use_container_width=True):
        st.cache_data.clear()
        st.success("Cach√© limpiado")

with col_c:
    if st.button("‚ùå Reset Completo", use_container_width=True):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.success("Sistema reiniciado")
        st.rerun()

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    üí° Desarrollado con Streamlit | Python | spaCy | scikit-learn
</div>
""", unsafe_allow_html=True)
```

---

## FASE 3: Streamlit App (Continuaci√≥n)

### **P√°gina 1: Upload de PDFs**

#### **`app/pages/1_üìÅ_Subir_PDFs.py`**

```python
"""
P√°gina de carga y procesamiento de PDFs.
"""
import streamlit as st
from app.utils.session_manager import init_session_state, set_state
from app.utils.cache_manager import cached_pdf_extraction, cached_text_processing
import sys
from pathlib import Path
from datetime import datetime

# Setup path
root_dir = Path(__file__).parent.parent.parent
sys.path.append(str(root_dir))

# Inicializar
init_session_state()

st.title("üìÅ Subir Programas de Estudio")

st.markdown("""
Sube los PDFs de programas de estudio que quieres analizar. 
Puedes subir m√∫ltiples archivos a la vez.
""")

# Upload widget
uploaded_files = st.file_uploader(
    "Arrastra archivos PDF o haz clic para seleccionar",
    type=['pdf'],
    accept_multiple_files=True,
    help="M√°ximo 50MB por archivo"
)

if uploaded_files:
    st.markdown("---")
    st.subheader(f"üìã Archivos Recibidos: {len(uploaded_files)}")
    
    # Tabla de archivos
    file_data = []
    for file in uploaded_files:
        size_mb = len(file.getvalue()) / (1024 * 1024)
        file_data.append({
            'Archivo': file.name,
            'Tama√±o (MB)': f"{size_mb:.2f}",
            'Estado': '‚è≥ Nuevo' if file.name not in st.session_state.uploaded_pdfs else '‚úÖ Ya cargado'
        })
    
    st.table(file_data)
    
    # Metadata adicional
    st.markdown("### üìù Informaci√≥n Adicional (Opcional)")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        programa = st.text_input(
            "Programa",
            placeholder="Ej: Ingenier√≠a en Computaci√≥n",
            help="Programa acad√©mico"
        )
    
    with col2:
        facultad = st.text_input(
            "Facultad",
            placeholder="Ej: Ciencias",
            help="Facultad o departamento"
        )
    
    with col3:
        a√±o = st.number_input(
            "A√±o",
            min_value=2000,
            max_value=2030,
            value=datetime.now().year,
            help="A√±o del programa"
        )
    
    # Bot√≥n de procesamiento
    st.markdown("---")
    
    col_btn1, col_btn2, col_btn3 = st.columns([1, 1, 2])
    
    with col_btn1:
        process_button = st.button(
            "üöÄ Procesar PDFs",
            type="primary",
            use_container_width=True
        )
    
    with col_btn2:
        skip_existing = st.checkbox(
            "Saltar ya procesados",
            value=True,
            help="No reprocesar PDFs ya cargados"
        )
    
    if process_button:
        # Metadata com√∫n
        common_metadata = {}
        if programa:
            common_metadata['programa'] = programa
        if facultad:
            common_metadata['facultad'] = facultad
        if a√±o:
            common_metadata['a√±o'] = str(a√±o)
        
        # Progress tracking
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_container = st.container()
        
        total_files = len(uploaded_files)
        success_count = 0
        error_count = 0
        
        for idx, uploaded_file in enumerate(uploaded_files):
            filename = uploaded_file.name
            
            # Skip si ya existe
            if skip_existing and filename in st.session_state.uploaded_pdfs:
                status_text.info(f"‚è≠Ô∏è Saltando {filename} (ya procesado)")
                continue
            
            try:
                # Actualizar status
                status_text.text(f"üìÑ Procesando {filename}... ({idx+1}/{total_files})")
                
                # 1. Guardar bytes
                pdf_bytes = uploaded_file.getvalue()
                st.session_state.uploaded_pdfs[filename] = pdf_bytes
                
                # 2. Extraer texto (con cach√©)
                with st.spinner(f"Extrayendo texto de {filename}..."):
                    extracted = cached_pdf_extraction(pdf_bytes, filename)
                    
                    # Agregar metadata adicional
                    extracted.metadata.update(common_metadata)
                    
                    # Guardar
                    st.session_state.extracted_texts[filename] = extracted
                
                # 3. Procesar texto (con cach√©)
                with st.spinner(f"Procesando texto de {filename}..."):
                    # Convertir a dict para cach√©
                    extracted_dict = extracted.to_dict()
                    extracted_dict['raw_text'] = extracted.raw_text  # Agregar texto completo
                    
                    processed = cached_text_processing(extracted_dict)
                    st.session_state.processed_texts[filename] = processed
                
                # Success
                with results_container:
                    st.success(f"‚úÖ {filename} procesado correctamente")
                    with st.expander(f"üìä Estad√≠sticas de {filename}"):
                        col1, col2, col3 = st.columns(3)
                        col1.metric("P√°ginas", extracted.page_count)
                        col2.metric("Tokens", len(processed.tokens))
                        col3.metric("Vocabulario", len(set(processed.tokens)))
                
                success_count += 1
                
            except Exception as e:
                error_count += 1
                with results_container:
                    st.error(f"‚ùå Error procesando {filename}: {str(e)}")
                
                # Log error
                import logging
                logging.error(f"Error processing {filename}: {str(e)}", exc_info=True)
            
            # Actualizar progress
            progress_bar.progress((idx + 1) / total_files)
        
        # Resumen final
        status_text.empty()
        progress_bar.empty()
        
        st.markdown("---")
        st.subheader("üìä Resumen de Procesamiento")
        
        col1, col2, col3 = st.columns(3)
        col1.metric("Total", total_files)
        col2.metric("Exitosos", success_count, delta=success_count)
        col3.metric("Errores", error_count, delta=-error_count if error_count > 0 else None)
        
        if success_count > 0:
            st.success(f"üéâ {success_count} documentos listos para an√°lisis!")
            st.info("üëâ Ve a **üìä An√°lisis de Frecuencias** para explorar los datos")

# Secci√≥n de documentos ya cargados
if st.session_state.uploaded_pdfs:
    st.markdown("---")
    st.subheader("üìö Documentos en el Sistema")
    
    # Filtros
    col1, col2 = st.columns([3, 1])
    
    with col1:
        search = st.text_input(
            "üîç Buscar documento",
            placeholder="Nombre del archivo..."
        )
    
    with col2:
        show_details = st.checkbox("Ver detalles", value=False)
    
    # Lista de documentos
    for filename in st.session_state.uploaded_pdfs.keys():
        # Filtro de b√∫squeda
        if search and search.lower() not in filename.lower():
            continue
        
        with st.expander(f"üìÑ {filename}", expanded=False):
            # Estado
            is_extracted = filename in st.session_state.extracted_texts
            is_processed = filename in st.session_state.processed_texts
            is_analyzed = filename in st.session_state.frequency_analyses
            
            col1, col2, col3 = st.columns(3)
            col1.write(f"**Extra√≠do:** {'‚úÖ' if is_extracted else '‚ùå'}")
            col2.write(f"**Procesado:** {'‚úÖ' if is_processed else '‚ùå'}")
            col3.write(f"**Analizado:** {'‚úÖ' if is_analyzed else '‚ùå'}")
            
            # Detalles
            if show_details and is_extracted:
                extracted = st.session_state.extracted_texts[filename]
                st.json(extracted.metadata)
            
            # Acciones
            col_a, col_b, col_c = st.columns(3)
            
            with col_a:
                if st.button(f"üîÑ Reprocesar", key=f"reprocess_{filename}"):
                    # Limpiar cach√© de este archivo
                    if filename in st.session_state.processed_texts:
                        del st.session_state.processed_texts[filename]
                    if filename in st.session_state.frequency_analyses:
                        del st.session_state.frequency_analyses[filename]
                    st.success(f"Listo para reprocesar {filename}")
                    st.rerun()
            
            with col_b:
                if is_processed:
                    processed = st.session_state.processed_texts[filename]
                    if st.button(f"üëÅÔ∏è Preview", key=f"preview_{filename}"):
                        st.text_area(
                            "Texto Procesado (primeros 500 caracteres)",
                            processed.clean_text[:500],
                            height=150,
                            disabled=True
                        )
            
            with col_c:
                if st.button(f"üóëÔ∏è Eliminar", key=f"delete_{filename}"):
                    # Eliminar de todos los estados
                    del st.session_state.uploaded_pdfs[filename]
                    if filename in st.session_state.extracted_texts:
                        del st.session_state.extracted_texts[filename]
                    if filename in st.session_state.processed_texts:
                        del st.session_state.processed_texts[filename]
                    if filename in st.session_state.frequency_analyses:
                        del st.session_state.frequency_analyses[filename]
                    st.success(f"Eliminado {filename}")
                    st.rerun()

# Acciones globales
if st.session_state.uploaded_pdfs:
    st.markdown("---")
    st.subheader("‚öôÔ∏è Acciones Globales")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üîÑ Reprocesar Todos", use_container_width=True):
            st.session_state.processed_texts = {}
            st.session_state.frequency_analyses = {}
            st.success("Todos los documentos listos para reprocesar")
            st.rerun()
    
    with col2:
        if st.button("üóëÔ∏è Eliminar Todos", use_container_width=True, type="secondary"):
            if st.checkbox("Confirmar eliminaci√≥n de TODOS los documentos"):
                st.session_state.uploaded_pdfs = {}
                st.session_state.extracted_texts = {}
                st.session_state.processed_texts = {}
                st.session_state.frequency_analyses = {}
                st.success("Todos los documentos eliminados")
                st.rerun()

# Tips
with st.sidebar:
    st.markdown("### üí° Tips")
    st.info("""
    - Los PDFs deben contener texto seleccionable (no escaneados)
    - Tama√±o m√°ximo: 50MB por archivo
    - Formatos soportados: PDF
    - Los datos se procesan con cach√© para mayor velocidad
    """)
```

---

### **P√°gina 2: An√°lisis de Frecuencias**

#### **`app/pages/2_üìä_Analisis_Frecuencias.py`**

```python
"""
P√°gina de an√°lisis de frecuencias y t√©rminos.
"""
import streamlit as st
from app.utils.session_manager import init_session_state
from app.utils.cache_manager import cached_frequency_analysis
from src.visualization.plotly_charts import PlotlyCharts
from src.visualization.wordclouds import WordCloudGenerator
import pandas as pd
import sys
from pathlib import Path

root_dir = Path(__file__).parent.parent.parent
sys.path.append(str(root_dir))

init_session_state()

st.title("üìä An√°lisis de Frecuencias")

# Verificar que hay documentos procesados
if not st.session_state.processed_texts:
    st.warning("‚ö†Ô∏è No hay documentos procesados. Ve a **üìÅ Subir PDFs** primero.")
    st.stop()

# Sidebar: Filtros y configuraci√≥n
with st.sidebar:
    st.header("üéõÔ∏è Configuraci√≥n")
    
    # Selecci√≥n de documentos
    available_docs = list(st.session_state.processed_texts.keys())
    selected_docs = st.multiselect(
        "Documentos a analizar",
        options=available_docs,
        default=available_docs[:min(3, len(available_docs))],
        help="Selecciona uno o m√°s documentos"
    )
    
    if not selected_docs:
        st.error("Debes seleccionar al menos un documento")
        st.stop()
    
    st.markdown("---")
    
    # Par√°metros de visualizaci√≥n
    st.subheader("Visualizaci√≥n")
    
    top_n = st.slider(
        "Top N t√©rminos",
        min_value=10,
        max_value=100,
        value=30,
        step=5
    )
    
    ngram_type = st.selectbox(
        "Tipo de N-gram",
        options=[1, 2, 3],
        format_func=lambda x: f"{x}-gram" + ("s (palabras simples)" if x==1 else "s (frases)")
    )
    
    st.markdown("---")
    
    # An√°lisis comparativo
    compare_mode = st.checkbox(
        "Modo comparaci√≥n",
        value=len(selected_docs) > 1,
        help="Compara t√©rminos entre documentos"
    )

# Ejecutar an√°lisis
st.subheader("üîÑ Ejecutando An√°lisis...")

progress_bar = st.progress(0)
analyses = {}

for idx, doc_name in enumerate(selected_docs):
    processed = st.session_state.processed_texts[doc_name]
    
    # Usar cach√©
    processed_dict = processed.to_dict()
    processed_dict['clean_text'] = processed.clean_text
    processed_dict['tokens'] = processed.tokens
    processed_dict['lemmas'] = processed.lemmas
    processed_dict['pos_tags'] = processed.pos_tags
    processed_dict['entities'] = processed.entities
    
    analysis = cached_frequency_analysis(processed_dict)
    analyses[doc_name] = analysis
    
    # Guardar en session state
    st.session_state.frequency_analyses[doc_name] = analysis
    
    progress_bar.progress((idx + 1) / len(selected_docs))

progress_bar.empty()
st.success(f"‚úÖ {len(selected_docs)} documentos analizados")

# Inicializar generadores de visualizaciones
charts = PlotlyCharts()
wordcloud_gen = WordCloudGenerator()

# Tabs principales
tab1, tab2, tab3, tab4 = st.tabs([
    "üìä Gr√°ficos de Barras",
    "‚òÅÔ∏è Nube de Palabras",
    "üìã Tablas Detalladas",
    "üîó Co-ocurrencias"
])

# TAB 1: Gr√°ficos de Barras
with tab1:
    st.subheader("T√©rminos M√°s Relevantes (TF-IDF)")
    
    if compare_mode and len(selected_docs) > 1:
        # Gr√°ficos lado a lado
        cols = st.columns(min(len(selected_docs), 2))
        
        for idx, (doc_name, analysis) in enumerate(analyses.items()):
            with cols[idx % 2]:
                st.markdown(f"**{doc_name}**")
                fig = charts.plot_top_terms(
                    analysis.term_frequencies,
                    top_n=top_n,
                    title=""
                )
                st.plotly_chart(fig, use_container_width=True)
    else:
        # Un solo documento
        doc_name = selected_docs[0]
        analysis = analyses[doc_name]
        
        fig = charts.plot_top_terms(
            analysis.term_frequencies,
            top_n=top_n,
            title=f"Top {top_n} T√©rminos - {doc_name}"
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # N-grams
    st.markdown("---")
    st.subheader(f"Top {ngram_type}-grams")
    
    for doc_name, analysis in analyses.items():
        if ngram_type in analysis.ngrams:
            df_ngrams = analysis.ngrams[ngram_type].head(20)
            
            if not df_ngrams.empty:
                st.markdown(f"**{doc_name}**")
                
                fig = charts.plot_top_terms(
                    df_ngrams.rename(columns={'ngram': 'term'}),
                    top_n=20,
                    title=""
                )
                st.plotly_chart(fig, use_container_width=True)

# TAB 2: Nube de Palabras
with tab2:
    st.subheader("Nubes de Palabras")
    
    # Configuraci√≥n
    col1, col2 = st.columns(2)
    with col1:
        colormap = st.selectbox(
            "Esquema de colores",
            options=['viridis', 'plasma', 'inferno', 'magma', 'cividis', 'Blues', 'Greens'],
            index=0
        )
    
    with col2:
        bg_color = st.selectbox(
            "Color de fondo",
            options=['white', 'black'],
            index=0
        )
    
    # Generar wordclouds
    cols = st.columns(min(len(selected_docs), 2))
    
    for idx, (doc_name, analysis) in enumerate(analyses.items()):
        processed = st.session_state.processed_texts[doc_name]
        
        with cols[idx % 2]:
            st.markdown(f"**{doc_name}**")
            
            with st.spinner("Generando nube de palabras..."):
                # Usar texto procesado
                wc_image = wordcloud_gen.generate(
                    processed.clean_text,
                    colormap=colormap,
                    background_color=bg_color
                )
                
                st.image(wc_image, use_container_width=True)
            
            # Bot√≥n de descarga
            wc_bytes = wordcloud_gen.to_bytes(wc_image)
            st.download_button(
                label="üíæ Descargar PNG",
                data=wc_bytes,
                file_name=f"wordcloud_{doc_name}.png",
                mime="image/png",
                key=f"download_wc_{doc_name}"
            )

# TAB 3: Tablas Detalladas
with tab3:
    st.subheader("Datos Tabulares")
    
    # Selector de documento (para tablas)
    if len(selected_docs) > 1:
        table_doc = st.selectbox(
            "Selecciona documento",
            options=selected_docs,
            key="table_selector"
        )
    else:
        table_doc = selected_docs[0]
    
    analysis = analyses[table_doc]
    
    # M√©tricas r√°pidas
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Vocabulario", analysis.vocabulary_size)
    col2.metric("Top T√©rminos", len(analysis.top_terms))
    col3.metric("Unigrams", len(analysis.ngrams.get(1, [])))
    col4.metric("Bigrams", len(analysis.ngrams.get(2, [])))
    
    st.markdown("---")
    
    # Tabla de frecuencias
    st.markdown("### üìä Tabla de Frecuencias")
    
    # Filtros
    col_f1, col_f2 = st.columns(2)
    with col_f1:
        min_freq = st.number_input(
            "Frecuencia m√≠nima",
            min_value=1,
            value=2,
            step=1
        )
    
    with col_f2:
        sort_by = st.selectbox(
            "Ordenar por",
            options=['tfidf', 'frequency'],
            format_func=lambda x: 'TF-IDF' if x == 'tfidf' else 'Frecuencia'
        )
    
    # Filtrar y mostrar
    df_display = analysis.term_frequencies[
        analysis.term_frequencies['frequency'] >= min_freq
    ].sort_values(sort_by, ascending=False)
    
    st.dataframe(
        df_display,
        use_container_width=True,
        height=400
    )
    
    # Exportar
    csv = df_display.to_csv(index=False).encode('utf-8')
    st.download_button(
        label="üì• Descargar CSV",
        data=csv,
        file_name=f"frecuencias_{table_doc}.csv",
        mime="text/csv"
    )

# TAB 4: Co-ocurrencias
with tab4:
    st.subheader("Co-ocurrencias de T√©rminos")
    
    st.info("""
    Las co-ocurrencias muestran qu√© t√©rminos aparecen frecuentemente juntos,
    lo que puede revelar conceptos relacionados.
    """)
    
    # Selector de documento
    if len(selected_docs) > 1:
        cooccur_doc = st.selectbox(
            "Selecciona documento",
            options=selected_docs,
            key="cooccur_selector"
        )
    else:
        cooccur_doc = selected_docs[0]
    
    processed = st.session_state.processed_texts[cooccur_doc]
    
    # Par√°metros
    col1, col2 = st.columns(2)
    with col1:
        window_size = st.slider(
            "Tama√±o de ventana",
            min_value=2,
            max_value=10,
            value=5,
            help="Distancia m√°xima entre palabras para considerarlas relacionadas"
        )
    
    with col2:
        top_cooccur = st.slider(
            "Top co-ocurrencias",
            min_value=10,
            max_value=50,
            value=30
        )
    
    # Calcular co-ocurrencias
    with st.spinner("Calculando co-ocurrencias..."):
        from src.analysis.frequency import FrequencyAnalyzer
        analyzer = FrequencyAnalyzer()
        
        df_cooccur = analyzer.get_cooccurrences(processed, window_size=window_size)
    
    # Visualizaci√≥n
    if not df_cooccur.empty:
        # Tabla
        st.dataframe(
            df_cooccur.head(top_cooccur),
            use_container_width=True
        )
        
        # Gr√°fico de red (simplificado)
        st.markdown("### üï∏Ô∏è Red de Co-ocurrencias")
        
        fig = charts.plot_cooccurrence_network(df_cooccur, top_n=min(30, top_cooccur))
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("No se encontraron co-ocurrencias significativas")

# Comparaci√≥n entre documentos (si modo comparaci√≥n)
if compare_mode and len(selected_docs) > 1:
    st.markdown("---")
    st.subheader("üìà Comparaci√≥n entre Documentos")
    
    # Matriz de similitud de vocabulario
    from src.analysis.frequency import FrequencyAnalyzer
    analyzer = FrequencyAnalyzer()
    
    analyses_list = [analyses[doc] for doc in selected_docs]
    comparison_df = analyzer.compare_documents(analyses_list)
    
    # Heatmap
    fig = charts.plot_skills_heatmap(
        comparison_df.T,  # Transponer para mejor visualizaci√≥n
        title="Similitud de T√©rminos entre Documentos (TF-IDF)"
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # T√©rminos √∫nicos vs compartidos
    st.markdown("### üîç An√°lisis de T√©rminos √önicos")
    
    all_terms = set()
    doc_terms = {}
    
    for doc_name in selected_docs:
        terms = set(analyses[doc_name].term_frequencies['term'])
        doc_terms[doc_name] = terms
        all_terms.update(terms)
    
    # T√©rminos compartidos por todos
    shared_terms = set.intersection(*doc_terms.values())
    
    col1, col2 = st.columns(2)
    col1.metric("T√©rminos totales √∫nicos", len(all_terms))
    col2.metric("T√©rminos compartidos", len(shared_terms))
    
    # Mostrar t√©rminos compartidos
    if shared_terms:
        with st.expander(f"Ver {len(shared_terms)} t√©rminos compartidos"):
            st.write(sorted(list(shared_terms)))
    
    # T√©rminos √∫nicos por documento
    for doc_name in selected_docs:
        unique = doc_terms[doc_name] - set.union(*[doc_terms[d] for d in selected_docs if d != doc_name])
        
        if unique:
            with st.expander(f"üìÑ {doc_name}: {len(unique)} t√©rminos √∫nicos"):
                st.write(sorted(list(unique))[:50])  # Primeros 50

# Footer con tips
with st.sidebar:
    st.markdown("---")
    st.markdown("### üí° Interpretaci√≥n")
    st.info("""
    **TF-IDF**: Mide importancia relativa del t√©rmino en el documento vs el corpus.
    
    **Frecuencia**: Conteo simple de apariciones.
    
    **N-grams**: Frases de N palabras que aparecen juntas.
    
    **Co-ocurrencias**: Palabras que aparecen cerca una de otra.
    """)
```

---

### **P√°gina 3: Topics y Habilidades**

#### **`app/pages/3_üéØ_Topics_Habilidades.py`**

```python
"""
P√°gina de topic modeling y mapeo de habilidades.
"""
import streamlit as st
from app.utils.session_manager import init_session_state
from app.utils.cache_manager import cached_topic_modeling
from src.visualization.plotly_charts import PlotlyCharts
import pandas as pd
import sys
from pathlib import Path

root_dir = Path(__file__).parent.parent.parent
sys.path.append(str(root_dir))

init_session_state()

st.title("üéØ Topics y Habilidades")

# Verificar prerequisitos
if not st.session_state.processed_texts:
    st.warning("‚ö†Ô∏è No hay documentos procesados.")
    st.stop()

if not st.session_state.frequency_analyses:
    st.warning("‚ö†Ô∏è Ejecuta el an√°lisis de frecuencias primero.")
    st.stop()

# Tabs principales
tab1, tab2, tab3 = st.tabs([
    "üîç Topic Modeling",
    "üéØ Mapeo de Habilidades",
    "‚öôÔ∏è Configurar Taxonom√≠a"
])

# TAB 1: Topic Modeling
with tab1:
    st.subheader("Descubrimiento Autom√°tico de Topics")
    
    st.markdown("""
    El topic modeling identifica autom√°ticamente temas recurrentes en los documentos.
    Cada topic es una distribuci√≥n de palabras que aparecen juntas frecuentemente.
    """)
    
    # Sidebar para topic modeling
    with st.sidebar:
        st.header("üéõÔ∏è Configuraci√≥n de Topics")
        
        n_topics = st.slider(
            "N√∫mero de topics",
            min_value=3,
            max_value=20,
            value=10,
            help="M√°s topics = m√°s granularidad"
        )
        
        method = st.selectbox(
            "M√©todo",
            options=['lda', 'nmf'],
            format_func=lambda x: 'LDA (Latent Dirichlet Allocation)' if x=='lda' else 'NMF (Non-negative Matrix Factorization)'
        )
        
        auto_label = st.checkbox(
            "Auto-etiquetar topics",
            value=True,
            help="Genera etiquetas autom√°ticas basadas en keywords"
        )
        
        run_modeling = st.button("üöÄ Ejecutar Topic Modeling", type="primary")
    
    # Ejecutar modeling
    if run_modeling or st.session_state.topic_model is not None:
        
        if run_modeling:
            # Preparar documentos
            processed_dicts = []
            for doc_name in st.session_state.processed_texts.keys():
                processed = st.session_state.processed_texts[doc_name]
                processed_dict = processed.to_dict()
                processed_dict['clean_text'] = processed.clean_text
                processed_dict['tokens'] = processed.tokens
                processed_dict['lemmas'] = processed.lemmas
                processed_dict['pos_tags'] = processed.pos_tags
                processed_dict['entities'] = processed.entities
                processed_dicts.append(processed_dict)
            
            with st.spinner(f"Entrenando modelo {method.upper()} con {n_topics} topics..."):
                topic_result = cached_topic_modeling(processed_dicts, n_topics, method)
                
                # Auto-etiquetar si est√° activado
                if auto_label:
                    from src.analysis.topics import TopicModeler
                    modeler = TopicModeler()
                    topic_result = modeler.auto_label_topics(topic_result)
                
                # Guardar en session state
                st.session_state.topic_model = topic_result
            
            st.success(f"‚úÖ Modelo {method.upper()} entrenado con {n_topics} topics")
        
        topic_result = st.session_state.topic_model
        
        # M√©tricas del modelo
        col1, col2, col3 = st.columns(3)
        col1.metric("Topics", topic_result.n_topics)
        col2.metric("Coherence Score", f"{topic_result.coherence_score:.3f}")
        if topic_result.perplexity:
            col3.metric("Perplexity", f"{topic_result.perplexity:.1f}")
        
        st.markdown("---")
        
        # Mostrar topics
        st.subheader("üìã Topics Identificados")
        
        for topic in topic_result.topics:
            with st.expander(f"**Topic {topic.topic_id}: {topic.label}**", expanded=True):
                # Keywords con pesos
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown("**Top Keywords:**")
                    keywords_df = pd.DataFrame({
                        'Keyword': topic.keywords[:10],
                        'Peso': [f"{w:.3f}" for w in topic.weights[:10]]
                    })
                    st.dataframe(keywords_df, use_container_width=True, hide_index=True)
                
                with col2:
                    st.markdown("**Documentos asociados:**")
                    st.write(f"Total: {len(topic.documents)}")
                    if topic.documents:
                        for doc in topic.documents[:5]:
                            st.write(f"- {doc}")
                        if len(topic.documents) > 5:
                            st.write(f"... y {len(topic.documents)-5} m√°s")
                
                # Editar etiqueta
                new_label = st.text_input(
                    "Renombrar topic:",
                    value=topic.label,
                    key=f"label_{topic.topic_id}"
                )
                
                if new_label != topic.label:
                    topic.label = new_label
                    st.success(f"Topic renombrado a: {new_label}")
        
        # Visualizaci√≥n de distribuci√≥n de topics
        st.markdown("---")
        st.subheader("üìä Distribuci√≥n de Topics en Documentos")
        
        charts = PlotlyCharts()
        
        # Preparar labels
        topic_labels = [t.label for t in topic_result.topics]
        
        fig = charts.plot_topic_distribution(
            topic_result.document_topic_matrix,
            topic_labels
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Tabla de doc-topic matrix
        with st.expander("üìã Ver Matriz Documento-Topic"):
            # Redondear valores
            display_matrix = topic_result.document_topic_matrix.round(3)
            display_matrix.columns = topic_labels
            
            st.dataframe(display_matrix, use_container_width=True)
            
            # Exportar
            csv = display_matrix.to_csv().encode('utf-8')
            st.download_button(
                "üì• Descargar CSV",
                data=csv,
                file_name="doc_topic_matrix.csv",
                mime="text/csv"
            )
    
    else:
        st.info("üëà Configura los par√°metros en el sidebar y presiona **Ejecutar Topic Modeling**")

# TAB 2: Mapeo de Habilidades
with tab2:
    st.subheader("Mapeo de Habilidades")
    
    st.markdown("""
    Identifica qu√© habilidades se trabajan en cada programa bas√°ndose en la taxonom√≠a definida.
    """)
    
    # Bot√≥n para ejecutar mapeo
    if st.button("üéØ Mapear Habilidades", type="primary"):
        from src.analysis.skills_mapper import SkillsMapper
        
        mapper = SkillsMapper()
        
        with st.spinner("Mapeando habilidades..."):
            # Mapear cada documento
            for doc_name in st.session_state.frequency_analyses.keys():
                freq_analysis = st.session_state.frequency_analyses[doc_name]
                processed = st.session_state.processed_texts[doc_name]
                
                profile = mapper.map_document(freq_analysis, processed)
                st.session_state.skill_profiles[doc_name] = profile
        
        st.success(f"‚úÖ {len(st.session_state.skill_profiles)} documentos mapeados")
    
    # Mostrar resultados si existen
    if st.session_state.skill_profiles:
        st.markdown("---")
        
        # Selector de documento
        selected_doc = st.selectbox(
            "Selecciona documento",
            options=list(st.session_state.skill_profiles.keys())
        )
        
        profile = st.session_state.skill_profiles[selected_doc]
        
        # M√©tricas
        col1, col2, col3 = st.columns(3)
        col1.metric("Habilidades detectadas", len(profile.skill_scores))
        col2.metric("Habilidades relevantes", len(profile.top_skills))
        col3.metric("Cobertura", f"{profile.skill_coverage*100:.1f}%")
        
        # Top habilidades
        st.markdown("### üèÜ Top Habilidades")
        
        df_skills = profile.to_dataframe()
        
        # Filtrar por confidence m√≠nima
        min_confidence = st.slider(
            "Confianza m√≠nima",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.05
        )
        
        df_filtered = df_skills[df_skills['confidence'] >= min_confidence]
        
        # Visualizaci√≥n de barras
        if not df_filtered.empty:
            charts = PlotlyCharts()
            
            fig = charts.plot_top_terms(
                df_filtered.rename(columns={'skill': 'term', 'score': 'tfidf'}),
                top_n=20,
                title="Habilidades Identificadas (ordenadas por score)"
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Tabla detallada
            st.dataframe(df_filtered, use_container_width=True)
        else:
            st.warning("No hay habilidades que cumplan el criterio de confianza")
        
        # Matriz de habilidades (si hay m√∫ltiples documentos)
        if len(st.session_state.skill_profiles) > 1:
            st.markdown("---")
            st.subheader("üìä Matriz de Habilidades")
            
            from src.analysis.skills_mapper import SkillsMapper
            mapper = SkillsMapper()
            
            profiles_list = list(st.session_state.skill_profiles.values())
            skills_matrix = mapper.create_skills_matrix(profiles_list)
            
            # Heatmap
            fig = charts.plot_skills_heatmap(
                skills_matrix,
                title="Habilidades por Documento"
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # Tabla
            with st.expander("üìã Ver Tabla Completa"):
                st.dataframe(skills_matrix.round(3), use_container_width=True)
                
                # Exportar
                csv = skills_matrix.to_csv().encode('utf-8')
                st.download_button(
                    "üì• Descargar Matriz CSV",
                    data=csv,
                    file_name="skills_matrix.csv",
                    mime="text/csv"
                )
    
    else:
        st.info("Presiona **Mapear Habilidades** para comenzar")

# TAB 3: Configurar Taxonom√≠a
with tab3:
    st.subheader("‚öôÔ∏è Gesti√≥n de Taxonom√≠a de Habilidades")
    
    from src.analysis.skills_mapper import SkillsMapper
    mapper = SkillsMapper()
    
    st.markdown("""
    Define o modifica las habilidades que el sistema buscar√° en los documentos.
    Cada habilidad tiene keywords que se usar√°n para identificarla.
    """)
    
    # Mostrar taxonom√≠a actual
    st.markdown("### üìö Habilidades Actuales")
    
    # Convertir a DataFrame para visualizaci√≥n
    taxonomia_data = []
    for skill_id, data in st.session_state.taxonomia.items():
        taxonomia_data.append({
            'ID': skill_id,
            'Nombre': data['name'],
            'Keywords': ', '.join(data['keywords'][:5]) + ('...' if len(data['keywords']) > 5 else ''),
            'Categor√≠a': data.get('category', 'general'),
            'Peso': data.get('weight', 1.0)
        })
    
    df_taxonomia = pd.DataFrame(taxonomia_data)
    st.dataframe(df_taxonomia, use_container_width=True)
    
    st.markdown("---")
    
    # Agregar nueva habilidad
    st.markdown("### ‚ûï Agregar Nueva Habilidad")
    
    col1, col2 = st.columns(2)
    
    with col1:
        new_skill_id = st.text_input(
            "ID de habilidad",
            placeholder="ej: comunicacion_efectiva",
            help="Identificador √∫nico (sin espacios)"
        )
        
        new_skill_name = st.text_input(
            "Nombre",
            placeholder="ej: Comunicaci√≥n Efectiva"
        )
    
    with col2:
        new_skill_category = st.selectbox(
            "Categor√≠a",
            options=['cognitiva', 'tecnica', 'interpersonal', 'general']
        )
        
        new_skill_weight = st.number_input(
            "Peso",
            min_value=0.0,
            max_value=2.0,
            value=1.0,
            step=0.1
        )
    
    new_keywords = st.text_area(
        "Keywords (una por l√≠nea)",
        placeholder="comunicar\nexponer\npresentar\nredactar",
        height=100
    )
    
    new_synonyms = st.text_area(
        "Sin√≥nimos (una por l√≠nea)",
        placeholder="expresar\narticular",
        height=80
    )
    
    if st.button("‚ûï Agregar Habilidad"):
        if new_skill_id and new_skill_name and new_keywords:
            # Parsear keywords y synonyms
            keywords_list = [k.strip() for k in new_keywords.split('\n') if k.strip()]
            synonyms_list = [s.strip() for s in new_synonyms.split('\n') if s.strip()]
            
            # Agregar a taxonom√≠a
            st.session_state.taxonomia[new_skill_id] = {
                'name': new_skill_name,
                'keywords': keywords_list,
                'synonyms': synonyms_list,
                'weight': new_skill_weight,
                'category': new_skill_category
            }
            
            # Actualizar mapper
            mapper.update_taxonomia(st.session_state.taxonomia)
            
            st.success(f"‚úÖ Habilidad '{new_skill_name}' agregada")
            st.rerun()
        else:
            st.error("Debes completar al menos ID, Nombre y Keywords")
    
    st.markdown("---")
    
    # Editar/Eliminar habilidades existentes
    st.markdown("### ‚úèÔ∏è Editar Habilidades")
    
    skill_to_edit = st.selectbox(
        "Selecciona habilidad para editar",
        options=list(st.session_state.taxonomia.keys()),
        format_func=lambda x: st.session_state.taxonomia[x]['name']
    )
    
    if skill_to_edit:
        skill_data = st.session_state.taxonomia[skill_to_edit]
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            # Mostrar datos actuales
            st.json(skill_data)
        
        with col2:
            if st.button("üóëÔ∏è Eliminar", key="delete_skill"):
                del st.session_state.taxonomia[skill_to_edit]
                mapper.update_taxonomia(st.session_state.taxonomia)
                st.success(f"Habilidad eliminada")
                st.rerun()
    
    st.markdown("---")
    
    # Importar/Exportar
    st.markdown("### üì§üì• Importar/Exportar Taxonom√≠a")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Exportar
        import json
        taxonomia_json = json.dumps(st.session_state.taxonomia, indent=2, ensure_ascii=False)
        
        st.download_button(
            label="üì• Exportar JSON",
            data=taxonomia_json,
            file_name="taxonomia_habilidades.json",
            mime="application/json"
        )
    
    with col2:
        # Importar
        uploaded_taxonomia = st.file_uploader(
            "üì§ Importar JSON",
            type=['json'],
            help="Sube un archivo JSON con la estructura de taxonom√≠a"
        )
        
        if uploaded_taxonomia:
            import json
            new_taxonomia = json.load(uploaded_taxonomia)
            
            if st.button("‚úÖ Confirmar Importaci√≥n"):
                st.session_state.taxonomia = new_taxonomia
                mapper.update_taxonomia(new_taxonomia)
                st.success("Taxonom√≠a importada correctamente")
                st.rerun()
```

---

**(Contin√∫a en siguiente mensaje con P√°gina 4: Comparativa y P√°gina 5: Configuraci√≥n...)**## FASE 3: Streamlit App (Continuaci√≥n Final)

### **P√°gina 4: Comparativa entre Programas**

#### **`app/pages/4_üìà_Comparativa.py`**

```python
"""
P√°gina de comparaci√≥n entre programas/documentos.
"""
import streamlit as st
from app.utils.session_manager import init_session_state
from src.visualization.plotly_charts import PlotlyCharts
import pandas as pd
import numpy as np
import sys
from pathlib import Path

root_dir = Path(__file__).parent.parent.parent
sys.path.append(str(root_dir))

init_session_state()

st.title("üìà Comparativa entre Programas")

# Verificar prerequisitos
if not st.session_state.skill_profiles:
    st.warning("‚ö†Ô∏è Primero debes mapear habilidades. Ve a **üéØ Topics y Habilidades**.")
    st.stop()

st.markdown("""
Compara perfiles de habilidades, vocabulario y caracter√≠sticas entre diferentes programas.
""")

# Selector de programas a comparar
st.subheader("Selecci√≥n de Programas")

available_programs = list(st.session_state.skill_profiles.keys())

col1, col2 = st.columns([3, 1])

with col1:
    selected_programs = st.multiselect(
        "Programas a comparar (m√°ximo 4)",
        options=available_programs,
        default=available_programs[:min(2, len(available_programs))],
        max_selections=4,
        help="Selecciona entre 2 y 4 programas para comparar"
    )

with col2:
    comparison_type = st.selectbox(
        "Tipo de comparaci√≥n",
        options=['habilidades', 'vocabulario', 'topics', 'completa'],
        format_func=lambda x: {
            'habilidades': 'üéØ Habilidades',
            'vocabulario': 'üìö Vocabulario',
            'topics': 'üîç Topics',
            'completa': 'üìä Completa'
        }[x]
    )

if len(selected_programs) < 2:
    st.warning("‚ö†Ô∏è Selecciona al menos 2 programas para comparar")
    st.stop()

# Inicializar visualizaciones
charts = PlotlyCharts()

# Tabs seg√∫n tipo de comparaci√≥n
if comparison_type == 'completa':
    tabs = st.tabs([
        "üéØ Habilidades",
        "üìä Radar Chart",
        "üìö Vocabulario",
        "üîç Topics",
        "üìã Resumen"
    ])
else:
    tabs = [st.container()]

# ==================== TAB 1: HABILIDADES ====================
with tabs[0] if comparison_type == 'completa' else tabs[0]:
    if comparison_type in ['habilidades', 'completa']:
        st.subheader("üéØ Comparaci√≥n de Habilidades")
        
        # Crear matriz de habilidades
        from src.analysis.skills_mapper import SkillsMapper
        mapper = SkillsMapper()
        
        selected_profiles = [
            st.session_state.skill_profiles[prog] 
            for prog in selected_programs
        ]
        
        skills_matrix = mapper.create_skills_matrix(selected_profiles)
        
        # Filtro de habilidades
        min_score = st.slider(
            "Score m√≠nimo para mostrar",
            min_value=0.0,
            max_value=1.0,
            value=0.2,
            step=0.05,
            key="habilidades_min_score"
        )
        
        # Filtrar habilidades con score bajo en todos los programas
        skills_to_show = skills_matrix.index[
            (skills_matrix.max(axis=1) >= min_score)
        ].tolist()
        
        filtered_matrix = skills_matrix.loc[skills_to_show]
        
        # Heatmap
        if not filtered_matrix.empty:
            fig = charts.plot_skills_heatmap(
                filtered_matrix.T,  # Transponer: programas en filas
                title=f"Comparaci√≥n de Habilidades ({len(filtered_matrix)} habilidades)"
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # An√°lisis de diferencias
            st.markdown("### üìä An√°lisis de Diferencias")
            
            # Calcular estad√≠sticas por habilidad
            stats_data = []
            for skill in filtered_matrix.index:
                values = filtered_matrix.loc[skill].values
                stats_data.append({
                    'Habilidad': skill,
                    'Promedio': values.mean(),
                    'Desv. Std': values.std(),
                    'M√°ximo': values.max(),
                    'M√≠nimo': values.min(),
                    'Rango': values.max() - values.min()
                })
            
            df_stats = pd.DataFrame(stats_data).sort_values('Rango', ascending=False)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**Habilidades m√°s variables** (mayor diferencia)")
                st.dataframe(
                    df_stats.head(10)[['Habilidad', 'Rango', 'Desv. Std']],
                    use_container_width=True,
                    hide_index=True
                )
            
            with col2:
                st.markdown("**Habilidades m√°s uniformes** (menor diferencia)")
                st.dataframe(
                    df_stats[df_stats['Promedio'] > 0.1].nsmallest(10, 'Rango')[['Habilidad', 'Rango', 'Promedio']],
                    use_container_width=True,
                    hide_index=True
                )
            
            # Tabla detallada
            with st.expander("üìã Ver Tabla Completa de Habilidades"):
                display_matrix = filtered_matrix.T.round(3)
                st.dataframe(display_matrix, use_container_width=True)
                
                # Exportar
                csv = display_matrix.to_csv().encode('utf-8')
                st.download_button(
                    "üì• Descargar CSV",
                    data=csv,
                    file_name=f"comparativa_habilidades.csv",
                    mime="text/csv"
                )
        
        else:
            st.warning("No hay habilidades que cumplan el criterio de score m√≠nimo")

# ==================== TAB 2: RADAR CHART ====================
if comparison_type == 'completa':
    with tabs[1]:
        st.subheader("üìä Radar Chart de Habilidades")
        
        st.markdown("""
        Visualizaci√≥n radial que permite ver el perfil de habilidades de cada programa.
        Ideal para identificar fortalezas y debilidades relativas.
        """)
        
        # Selector de habilidades para el radar
        top_n_skills = st.slider(
            "N√∫mero de habilidades a mostrar",
            min_value=5,
            max_value=15,
            value=8,
            help="M√°s habilidades puede hacer el gr√°fico dif√≠cil de leer"
        )
        
        # Seleccionar top habilidades por promedio
        avg_scores = skills_matrix.mean(axis=1).sort_values(ascending=False)
        top_skills = avg_scores.head(top_n_skills).index.tolist()
        
        radar_matrix = skills_matrix.loc[top_skills].T
        
        # Generar radar chart
        fig = charts.plot_radar_skills(
            radar_matrix,
            selected_programs,
            title=f"Perfil de Top {top_n_skills} Habilidades"
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Interpretaci√≥n
        st.markdown("### üí° Interpretaci√≥n")
        
        for prog in selected_programs:
            prog_scores = radar_matrix.loc[prog]
            top_3 = prog_scores.nlargest(3)
            
            st.markdown(f"**{prog}:**")
            st.write(f"- Fortalezas principales: {', '.join(top_3.index[:3])}")
            
            if len(prog_scores[prog_scores < 0.3]) > 0:
                weak = prog_scores[prog_scores < 0.3].index.tolist()
                st.write(f"- √Åreas de menor √©nfasis: {', '.join(weak[:3])}")

# ==================== TAB 3: VOCABULARIO ====================
with tabs[2 if comparison_type == 'completa' else 0]:
    if comparison_type in ['vocabulario', 'completa']:
        st.subheader("üìö Comparaci√≥n de Vocabulario")
        
        # Obtener an√°lisis de frecuencias
        if all(prog in st.session_state.frequency_analyses for prog in selected_programs):
            
            # Diagrama de Venn conceptual
            st.markdown("### üîµ T√©rminos √önicos vs Compartidos")
            
            # Calcular t√©rminos por programa
            program_terms = {}
            for prog in selected_programs:
                freq_analysis = st.session_state.frequency_analyses[prog]
                # Tomar t√©rminos con TF-IDF significativo
                significant_terms = freq_analysis.term_frequencies[
                    freq_analysis.term_frequencies['tfidf'] > 0.1
                ]['term'].tolist()
                program_terms[prog] = set(significant_terms)
            
            # M√©tricas
            col1, col2, col3, col4 = st.columns(4)
            
            all_terms = set.union(*program_terms.values())
            shared_all = set.intersection(*program_terms.values())
            
            col1.metric("Total t√©rminos √∫nicos", len(all_terms))
            col2.metric("Compartidos por todos", len(shared_all))
            col3.metric("Promedio por programa", 
                       int(np.mean([len(terms) for terms in program_terms.values()])))
            col4.metric("Overlap promedio", 
                       f"{(len(shared_all)/len(all_terms)*100):.1f}%")
            
            # Tabla de t√©rminos por programa
            st.markdown("### üìä An√°lisis de T√©rminos")
            
            terms_data = []
            for prog in selected_programs:
                unique = program_terms[prog] - set.union(
                    *[program_terms[p] for p in selected_programs if p != prog]
                )
                terms_data.append({
                    'Programa': prog,
                    'Total t√©rminos': len(program_terms[prog]),
                    '√önicos': len(unique),
                    '% √önicos': f"{(len(unique)/len(program_terms[prog])*100):.1f}%"
                })
            
            df_terms = pd.DataFrame(terms_data)
            st.dataframe(df_terms, use_container_width=True, hide_index=True)
            
            # Mostrar t√©rminos compartidos
            if shared_all:
                with st.expander(f"üîç Ver {len(shared_all)} t√©rminos compartidos por todos"):
                    shared_list = sorted(list(shared_all))
                    # Dividir en columnas
                    n_cols = 3
                    cols = st.columns(n_cols)
                    for i, term in enumerate(shared_list):
                        cols[i % n_cols].write(f"‚Ä¢ {term}")
            
            # T√©rminos √∫nicos por programa
            st.markdown("### üéØ T√©rminos Distintivos por Programa")
            
            for prog in selected_programs:
                unique = program_terms[prog] - set.union(
                    *[program_terms[p] for p in selected_programs if p != prog]
                )
                
                if unique:
                    with st.expander(f"{prog}: {len(unique)} t√©rminos √∫nicos"):
                        # Obtener TF-IDF de estos t√©rminos
                        freq_analysis = st.session_state.frequency_analyses[prog]
                        unique_df = freq_analysis.term_frequencies[
                            freq_analysis.term_frequencies['term'].isin(unique)
                        ].nlargest(20, 'tfidf')
                        
                        # Gr√°fico
                        fig = charts.plot_top_terms(
                            unique_df,
                            top_n=20,
                            title=f"Top 20 T√©rminos √önicos de {prog}"
                        )
                        st.plotly_chart(fig, use_container_width=True)
            
            # Comparaci√≥n de N-grams
            st.markdown("---")
            st.markdown("### üìù Comparaci√≥n de N-grams")
            
            ngram_type = st.selectbox(
                "Tipo de n-gram",
                options=[1, 2, 3],
                format_func=lambda x: f"{x}-grams",
                key="vocab_ngram_selector"
            )
            
            # Recopilar n-grams
            ngrams_dict = {}
            for prog in selected_programs:
                freq_analysis = st.session_state.frequency_analyses[prog]
                if ngram_type in freq_analysis.ngrams:
                    ngrams_dict[prog] = freq_analysis.ngrams[ngram_type]
            
            if ngrams_dict:
                fig = charts.plot_ngrams_comparison(ngrams_dict, n=ngram_type, top=15)
                st.plotly_chart(fig, use_container_width=True)
        
        else:
            st.warning("No todos los programas tienen an√°lisis de frecuencias")

# ==================== TAB 4: TOPICS ====================
with tabs[3 if comparison_type == 'completa' else 0]:
    if comparison_type in ['topics', 'completa']:
        st.subheader("üîç Comparaci√≥n de Topics")
        
        if st.session_state.topic_model is not None:
            topic_model = st.session_state.topic_model
            
            st.markdown("### üìä Distribuci√≥n de Topics por Programa")
            
            # Filtrar matriz para programas seleccionados
            doc_topic_filtered = topic_model.document_topic_matrix.loc[
                [prog for prog in selected_programs 
                 if prog in topic_model.document_topic_matrix.index]
            ]
            
            if not doc_topic_filtered.empty:
                # Preparar labels
                topic_labels = [t.label for t in topic_model.topics]
                
                # Gr√°fico de distribuci√≥n
                fig = charts.plot_topic_distribution(
                    doc_topic_filtered,
                    topic_labels
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # An√°lisis de topics dominantes
                st.markdown("### üèÜ Topics Dominantes por Programa")
                
                for prog in doc_topic_filtered.index:
                    topic_scores = doc_topic_filtered.loc[prog]
                    top_topics_idx = topic_scores.nlargest(3).index
                    
                    st.markdown(f"**{prog}:**")
                    for idx in top_topics_idx:
                        topic_num = int(idx.split('_')[1])
                        topic = topic_model.topics[topic_num]
                        score = topic_scores[idx]
                        
                        st.write(
                            f"- {topic.label} ({score:.2%}): "
                            f"{', '.join(topic.keywords[:5])}"
                        )
                
                # Heatmap de topics
                st.markdown("### üó∫Ô∏è Mapa de Topics")
                
                fig_heatmap = charts.plot_skills_heatmap(
                    doc_topic_filtered,
                    title="Intensidad de Topics por Programa"
                )
                st.plotly_chart(fig_heatmap, use_container_width=True)
            
            else:
                st.warning("Los programas seleccionados no est√°n en el modelo de topics")
        
        else:
            st.info("Primero debes ejecutar Topic Modeling en **üéØ Topics y Habilidades**")

# ==================== TAB 5: RESUMEN ====================
if comparison_type == 'completa':
    with tabs[4]:
        st.subheader("üìã Resumen Ejecutivo")
        
        st.markdown("""
        Resumen de las principales diferencias y similitudes entre los programas comparados.
        """)
        
        # Generar reporte autom√°tico
        st.markdown("### üìä An√°lisis Comparativo")
        
        for i, prog1 in enumerate(selected_programs):
            for prog2 in selected_programs[i+1:]:
                with st.expander(f"üîç {prog1} vs {prog2}"):
                    
                    profile1 = st.session_state.skill_profiles[prog1]
                    profile2 = st.session_state.skill_profiles[prog2]
                    
                    # Comparar perfiles
                    comparison_df = mapper.compare_profiles(profile1, profile2)
                    
                    # Principales diferencias
                    st.markdown("**Mayores diferencias:**")
                    top_diff = comparison_df.nlargest(5, 'difference')
                    
                    for skill in top_diff.index[:5]:
                        score1 = comparison_df.loc[skill, prog1]
                        score2 = comparison_df.loc[skill, prog2]
                        diff = comparison_df.loc[skill, 'difference']
                        
                        if score1 > score2:
                            st.write(
                                f"‚Ä¢ **{skill}**: {prog1} m√°s fuerte "
                                f"({score1:.2f} vs {score2:.2f}, diff: {diff:.2f})"
                            )
                        else:
                            st.write(
                                f"‚Ä¢ **{skill}**: {prog2} m√°s fuerte "
                                f"({score2:.2f} vs {score1:.2f}, diff: {diff:.2f})"
                            )
                    
                    # Similitudes
                    st.markdown("**Principales similitudes:**")
                    similarities = comparison_df.nsmallest(5, 'difference')
                    similarities = similarities[similarities[prog1] > 0.3]  # Solo significativas
                    
                    if not similarities.empty:
                        for skill in similarities.index[:5]:
                            score_avg = (comparison_df.loc[skill, prog1] + 
                                       comparison_df.loc[skill, prog2]) / 2
                            st.write(f"‚Ä¢ **{skill}**: ambos ~{score_avg:.2f}")
                    else:
                        st.write("No hay habilidades significativas en com√∫n")
        
        # Exportar reporte completo
        st.markdown("---")
        st.markdown("### üì• Exportar Reporte")
        
        if st.button("üìÑ Generar Reporte HTML"):
            # Generar HTML
            from src.visualization.reports import generate_comparison_report
            
            report_html = generate_comparison_report(
                programs=selected_programs,
                skills_matrix=skills_matrix,
                topic_model=st.session_state.topic_model,
                profiles=[st.session_state.skill_profiles[p] for p in selected_programs]
            )
            
            st.download_button(
                label="üíæ Descargar Reporte HTML",
                data=report_html,
                file_name=f"reporte_comparativo_{'-'.join(selected_programs)}.html",
                mime="text/html"
            )

# Sidebar con estad√≠sticas
with st.sidebar:
    st.markdown("### üìä Estad√≠sticas de Comparaci√≥n")
    
    st.metric("Programas comparados", len(selected_programs))
    
    if st.session_state.skill_profiles:
        total_skills = len(skills_matrix)
        st.metric("Habilidades evaluadas", total_skills)
    
    st.markdown("---")
    st.markdown("### üí° Tips")
    st.info("""
    - **Radar Chart**: Mejor para visualizar perfiles completos
    - **Heatmap**: Identifica patrones y clusters
    - **T√©rminos √∫nicos**: Revelan especializaci√≥n
    - **Topics**: Muestran enfoques tem√°ticos
    """)
```

---

### **P√°gina 5: Configuraci√≥n General**

#### **`app/pages/5_‚öôÔ∏è_Configuracion.py`**

```python
"""
P√°gina de configuraci√≥n del sistema.
"""
import streamlit as st
from app.utils.session_manager import init_session_state
import yaml
import sys
from pathlib import Path

root_dir = Path(__file__).parent.parent.parent
sys.path.append(str(root_dir))

init_session_state()

st.title("‚öôÔ∏è Configuraci√≥n del Sistema")

st.markdown("""
Ajusta los par√°metros de an√°lisis, gestiona datos y configura el comportamiento del sistema.
""")

# Tabs de configuraci√≥n
tab1, tab2, tab3, tab4 = st.tabs([
    "üéõÔ∏è Par√°metros NLP",
    "üìä An√°lisis",
    "üíæ Gesti√≥n de Datos",
    "üîß Sistema"
])

# ==================== TAB 1: PAR√ÅMETROS NLP ====================
with tab1:
    st.subheader("Configuraci√≥n de Procesamiento de Lenguaje Natural")
    
    config = st.session_state.config
    
    # Modelo spaCy
    st.markdown("### ü§ñ Modelo spaCy")
    
    current_model = config['nlp']['spacy_model']
    st.info(f"Modelo actual: **{current_model}**")
    
    new_model = st.selectbox(
        "Seleccionar modelo",
        options=[
            'es_core_news_sm',
            'es_core_news_md',
            'es_core_news_lg'
        ],
        index=['es_core_news_sm', 'es_core_news_md', 'es_core_news_lg'].index(current_model)
        if current_model in ['es_core_news_sm', 'es_core_news_md', 'es_core_news_lg']
        else 2,
        help="lg = m√°s preciso pero m√°s lento, sm = m√°s r√°pido pero menos preciso"
    )
    
    if new_model != current_model:
        if st.button("üîÑ Cambiar Modelo"):
            config['nlp']['spacy_model'] = new_model
            st.success(f"Modelo cambiado a {new_model}. Reinicia para aplicar cambios.")
    
    st.markdown("---")
    
    # Filtros de tokens
    st.markdown("### üîç Filtros de Tokens")
    
    col1, col2 = st.columns(2)
    
    with col1:
        min_length = st.number_input(
            "Longitud m√≠nima de palabra",
            min_value=1,
            max_value=10,
            value=config['nlp']['min_word_length'],
            help="Palabras m√°s cortas ser√°n ignoradas"
        )
    
    with col2:
        max_length = st.number_input(
            "Longitud m√°xima de palabra",
            min_value=10,
            max_value=50,
            value=config['nlp']['max_word_length'],
            help="Palabras m√°s largas ser√°n ignoradas"
        )
    
    remove_numbers = st.checkbox(
        "Remover n√∫meros",
        value=config['nlp']['remove_numbers'],
        help="Eliminar tokens num√©ricos del an√°lisis"
    )
    
    lemmatize = st.checkbox(
        "Aplicar lematizaci√≥n",
        value=config['nlp']['lemmatize'],
        help="Convertir palabras a su forma base (ej: corriendo ‚Üí correr)"
    )
    
    # POS tags
    st.markdown("### üè∑Ô∏è Etiquetas POS a Mantener")
    
    available_pos = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN']
    selected_pos = st.multiselect(
        "Tipos de palabras",
        options=available_pos,
        default=config['nlp']['pos_tags_keep'],
        help="Solo se mantendr√°n tokens de estos tipos"
    )
    
    st.markdown("---")
    
    # Stopwords custom
    st.markdown("### üö´ Stopwords Personalizadas")
    
    current_stopwords = config.get('stopwords_custom', [])
    
    st.info(f"Stopwords actuales: {len(current_stopwords)}")
    
    with st.expander("Ver/Editar Stopwords"):
        stopwords_text = st.text_area(
            "Stopwords (una por l√≠nea)",
            value='\n'.join(current_stopwords),
            height=200
        )
        
        if st.button("üíæ Guardar Stopwords"):
            new_stopwords = [s.strip() for s in stopwords_text.split('\n') if s.strip()]
            config['stopwords_custom'] = new_stopwords
            st.success(f"Guardadas {len(new_stopwords)} stopwords")
    
    # Bot√≥n para aplicar cambios NLP
    if st.button("‚úÖ Aplicar Cambios NLP", type="primary"):
        config['nlp']['min_word_length'] = min_length
        config['nlp']['max_word_length'] = max_length
        config['nlp']['remove_numbers'] = remove_numbers
        config['nlp']['lemmatize'] = lemmatize
        config['nlp']['pos_tags_keep'] = selected_pos
        
        st.session_state.config = config
        
        # Guardar a archivo
        config_path = Path(root_dir) / "config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True)
        
        st.success("‚úÖ Configuraci√≥n NLP actualizada")
        st.info("üí° Reprocesa los documentos para aplicar los cambios")

# ==================== TAB 2: AN√ÅLISIS ====================
with tab2:
    st.subheader("Configuraci√≥n de An√°lisis")
    
    # Frecuencias
    st.markdown("### üìä An√°lisis de Frecuencias")
    
    col1, col2 = st.columns(2)
    
    with col1:
        top_n_terms = st.number_input(
            "Top N t√©rminos",
            min_value=10,
            max_value=200,
            value=config['frequency']['top_n_terms'],
            step=10
        )
    
    with col2:
        tfidf_max_features = st.number_input(
            "Max features TF-IDF",
            min_value=100,
            max_value=2000,
            value=config['frequency']['tfidf_max_features'],
            step=100
        )
    
    ngram_min = st.number_input(
        "N-gram m√≠nimo",
        min_value=1,
        max_value=3,
        value=config['frequency']['ngram_range'][0]
    )
    
    ngram_max = st.number_input(
        "N-gram m√°ximo",
        min_value=1,
        max_value=5,
        value=config['frequency']['ngram_range'][1]
    )
    
    st.markdown("---")
    
    # Topic Modeling
    st.markdown("### üîç Topic Modeling")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        default_n_topics = st.number_input(
            "Topics por defecto",
            min_value=3,
            max_value=30,
            value=config['topics']['default_n_topics']
        )
    
    with col2:
        min_topics = st.number_input(
            "M√≠nimo topics",
            min_value=2,
            max_value=10,
            value=config['topics']['min_topics']
        )
    
    with col3:
        max_topics = st.number_input(
            "M√°ximo topics",
            min_value=10,
            max_value=50,
            value=config['topics']['max_topics']
        )
    
    lda_iterations = st.slider(
        "Iteraciones LDA",
        min_value=50,
        max_value=500,
        value=config['topics']['lda_iterations'],
        step=50
    )
    
    st.markdown("---")
    
    # Skills Mapping
    st.markdown("### üéØ Mapeo de Habilidades")
    
    col1, col2 = st.columns(2)
    
    with col1:
        min_confidence = st.slider(
            "Confianza m√≠nima",
            min_value=0.0,
            max_value=1.0,
            value=config['skills']['min_confidence'],
            step=0.05
        )
    
    with col2:
        weight_tfidf = st.slider(
            "Peso TF-IDF",
            min_value=0.0,
            max_value=1.0,
            value=config['skills']['weight_tfidf'],
            step=0.1
        )
    
    weight_frequency = 1.0 - weight_tfidf
    st.write(f"Peso Frecuencia: {weight_frequency:.1f}")
    
    # Aplicar cambios de an√°lisis
    if st.button("‚úÖ Aplicar Cambios de An√°lisis", type="primary"):
        config['frequency']['top_n_terms'] = top_n_terms
        config['frequency']['tfidf_max_features'] = tfidf_max_features
        config['frequency']['ngram_range'] = [ngram_min, ngram_max]
        
        config['topics']['default_n_topics'] = default_n_topics
        config['topics']['min_topics'] = min_topics
        config['topics']['max_topics'] = max_topics
        config['topics']['lda_iterations'] = lda_iterations
        
        config['skills']['min_confidence'] = min_confidence
        config['skills']['weight_tfidf'] = weight_tfidf
        config['skills']['weight_frequency'] = 1.0 - weight_tfidf
        
        st.session_state.config = config
        
        # Guardar
        config_path = Path(root_dir) / "config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True)
        
        st.success("‚úÖ Configuraci√≥n de an√°lisis actualizada")

# ==================== TAB 3: GESTI√ìN DE DATOS ====================
with tab3:
    st.subheader("Gesti√≥n de Datos y Cach√©")
    
    # Estad√≠sticas
    st.markdown("### üìä Estado Actual")
    
    col1, col2, col3, col4 = st.columns(4)
    
    col1.metric("PDFs", len(st.session_state.uploaded_pdfs))
    col2.metric("Procesados", len(st.session_state.processed_texts))
    col3.metric("Analizados", len(st.session_state.frequency_analyses))
    col4.metric("Perfiles", len(st.session_state.skill_profiles))
    
    st.markdown("---")
    
    # Limpieza selectiva
    st.markdown("### üóëÔ∏è Limpieza de Datos")
    
    st.warning("‚ö†Ô∏è Estas acciones son irreversibles")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üóëÔ∏è Limpiar An√°lisis", use_container_width=True):
            st.session_state.frequency_analyses = {}
            st.session_state.topic_model = None
            st.session_state.skill_profiles = {}
            st.success("An√°lisis limpiados (PDFs preservados)")
            st.rerun()
        
        if st.button("üóëÔ∏è Limpiar Procesados", use_container_width=True):
            st.session_state.processed_texts = {}
            st.session_state.frequency_analyses = {}
            st.session_state.topic_model = None
            st.session_state.skill_profiles = {}
            st.success("Textos procesados limpiados")
            st.rerun()
    
    with col2:
        if st.button("üóëÔ∏è Limpiar Cach√©", use_container_width=True):
            st.cache_data.clear()
            st.cache_resource.clear()
            st.success("Cach√© limpiado")
        
        if st.button("üóëÔ∏è RESET TOTAL", type="secondary", use_container_width=True):
            confirm = st.checkbox("Confirmar reset total de todos los datos")
            if confirm:
                for key in list(st.session_state.keys()):
                    del st.session_state[key]
                st.cache_data.clear()
                st.cache_resource.clear()
                st.success("Sistema reiniciado completamente")
                st.rerun()
    
    st.markdown("---")
    
    # Exportar/Importar datos
    st.markdown("### üíæ Backup y Restauraci√≥n")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**Exportar Datos**")
        
        if st.button("üì• Exportar Session State"):
            import json
            import pickle
            from datetime import datetime
            
            # Preparar datos exportables
            export_data = {
                'timestamp': datetime.now().isoformat(),
                'n_pdfs': len(st.session_state.uploaded_pdfs),
                'n_processed': len(st.session_state.processed_texts),
                'n_analyzed': len(st.session_state.frequency_analyses),
                'taxonomia': st.session_state.taxonomia,
                'config': st.session_state.config
            }
            
            json_data = json.dumps(export_data, indent=2, ensure_ascii=False)
            
            st.download_button(
                "üíæ Descargar Backup JSON",
                data=json_data,
                file_name=f"backup_{datetime.now():%Y%m%d_%H%M%S}.json",
                mime="application/json"
            )
    
    with col2:
        st.markdown("**Importar Datos**")
        
        uploaded_backup = st.file_uploader(
            "Selecciona archivo de backup",
            type=['json']
        )
        
        if uploaded_backup:
            import json
            backup_data = json.load(uploaded_backup)
            
            st.json(backup_data)
            
            if st.button("‚úÖ Restaurar Backup"):
                if 'taxonomia' in backup_data:
                    st.session_state.taxonomia = backup_data['taxonomia']
                if 'config' in backup_data:
                    st.session_state.config = backup_data['config']
                
                st.success("Backup restaurado")
                st.rerun()

# ==================== TAB 4: SISTEMA ====================
with tab4:
    st.subheader("Informaci√≥n del Sistema")
    
    # Info de versiones
    st.markdown("### üì¶ Versiones de Paquetes")
    
    import spacy
    import sklearn
    import pandas as pd
    import numpy as np
    import plotly
    
    versions = {
        'Python': sys.version.split()[0],
        'Streamlit': st.__version__,
        'spaCy': spacy.__version__,
        'scikit-learn': sklearn.__version__,
        'pandas': pd.__version__,
        'numpy': np.__version__,
        'plotly': plotly.__version__
    }
    
    df_versions = pd.DataFrame(list(versions.items()), columns=['Paquete', 'Versi√≥n'])
    st.dataframe(df_versions, use_container_width=True, hide_index=True)
    
    st.markdown("---")
    
    # Configuraci√≥n de UI
    st.markdown("### üé® Configuraci√≥n de UI")
    
    max_upload_size = st.number_input(
        "Tama√±o m√°ximo de upload (MB)",
        min_value=10,
        max_value=200,
        value=config['ui']['max_upload_size_mb']
    )
    
    cache_ttl = st.number_input(
        "TTL de cach√© (horas)",
        min_value=1,
        max_value=72,
        value=config['ui']['cache_ttl_hours']
    )
    
    if st.button("‚úÖ Aplicar Configuraci√≥n UI"):
        config['ui']['max_upload_size_mb'] = max_upload_size
        config['ui']['cache_ttl_hours'] = cache_ttl
        
        st.session_state.config = config
        
        config_path = Path(root_dir) / "config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True)
        
        st.success("Configuraci√≥n UI actualizada")
    
    st.markdown("---")
    
    # Logs y debug
    st.markdown("### üêõ Debug")
    
    if st.checkbox("Mostrar Session State"):
        st.json({
            k: str(v)[:100] + '...' if len(str(v)) > 100 else str(v)
            for k, v in st.session_state.items()
            if not k.startswith('_')
        })
    
    if st.checkbox("Mostrar Config Completo"):
        st.json(config)

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    ‚öôÔ∏è Configuraci√≥n del Sistema | Versi√≥n 1.0.0
</div>
""", unsafe_allow_html=True)
```

---

## FASE 4: Testing y Validaci√≥n - 3-5 d√≠as

### **Suite de Tests M√≠nima**

#### **`tests/test_integration.py`**

```python
"""
Tests de integraci√≥n del pipeline completo.
"""
import pytest
from pathlib import Path
import sys

root_dir = Path(__file__).parent.parent
sys.path.append(str(root_dir))

from src.extraction.pdf_extractor import PDFExtractor
from src.extraction.preprocessor import TextPreprocessor
from src.analysis.frequency import FrequencyAnalyzer
from src.analysis.topics import TopicModeler
from src.analysis.skills_mapper import SkillsMapper


def test_full_pipeline(mock_extracted_text, mock_taxonomia):
    """Test del pipeline completo end-to-end"""
    
    # 1. Preprocessing
    preprocessor = TextPreprocessor()
    processed = preprocessor.process(mock_extracted_text)
    
    assert len(processed.tokens) > 0
    assert len(processed.lemmas) == len(processed.tokens)
    
    # 2. Frequency Analysis
    analyzer = FrequencyAnalyzer()
    freq_analysis = analyzer.analyze_single(processed)
    
    assert not freq_analysis.term_frequencies.empty
    assert len(freq_analysis.top_terms) > 0
    
    # 3. Skills Mapping
    mapper = SkillsMapper()
    profile = mapper.map_document(freq_analysis, processed)
    
    assert len(profile.skill_scores) > 0
    assert 0 <= profile.skill_coverage <= 1.0


def test_multiple_documents_pipeline(mock_processed_text):
    """Test con m√∫ltiples documentos"""
    
    documents = [mock_processed_text] * 3
    
    # Topic modeling
    modeler = TopicModeler()
    result = modeler.fit(documents, n_topics=3, method='lda')
    
    assert result.n_topics == 3
    assert len(result.topics) == 3
    assert result.document_topic_matrix.shape == (3, 3)
```

---

## FASE 5: Deployment - 2-3 d√≠as

### **Opciones de Deploy**

#### **Opci√≥n 1: Streamlit Cloud (Recomendada)**

**`README.md` con instrucciones:**

```markdown
# An√°lisis de Programas de Estudio

## Deploy en Streamlit Cloud

1. Pushea el repo a GitHub
2. Ve a [share.streamlit.io](https://share.streamlit.io)
3. Conecta tu repo
4. Selecciona `app/Home.py` como main file
5. Deploy!

## Requisitos previos
- Descargar modelo spaCy: `python -m spacy download es_core_news_lg`

## Configuraci√≥n
- Edita `config.yaml` para ajustar par√°metros
- Personaliza taxonom√≠a en `data/taxonomia/habilidades.json`
```

#### **Opci√≥n 2: Docker**

**`Dockerfile`:**

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Instalar dependencias del sistema
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copiar requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Descargar modelo spaCy
RUN python -m spacy download es_core_news_lg

# Copiar aplicaci√≥n
COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "app/Home.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

**`docker-compose.yml`:**

```yaml
version: '3.8'

services:
  streamlit:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./outputs:/app/outputs
    environment:
      - STREAMLIT_SERVER_MAX_UPLOAD_SIZE=50
```

---

## RESUMEN EJECUTIVO DEL PLAN

### **Timeline Optimizado (6-7 semanas)**

| Fase | Duraci√≥n | Trabajo en Paralelo |
|------|----------|---------------------|
| **0. Setup** | 1 d√≠a | - |
| **1. Backend Core** | 1 semana | 4 tracks paralelos (A-D) |
| **2. Visualizaciones** | 3-4 d√≠as | Track E |
| **3. Streamlit App** | 1 semana | 5 p√°ginas en paralelo |
| **4. Testing** | 3-5 d√≠as | Tests + fixes |
| **5. Deploy** | 2-3 d√≠as | Documentaci√≥n + deploy |

### **Estrategia de Paralelizaci√≥n con Claude Code**

1. **Sesiones separadas por m√≥dulo** con contratos claros
2. **Datos mock compartidos** v√≠a `tests/conftest.py`
3. **Schemas estrictos** en `src/utils/schemas.py`
4. **Development iterativo:** MVP ‚Üí Features ‚Üí Polish

### **Archivos Cr√≠ticos a Crear Primero**

‚úÖ `config.yaml`  
‚úÖ `src/utils/schemas.py`  
‚úÖ `tests/conftest.py` (fixtures)  
‚úÖ `requirements.txt`  
‚úÖ `app/utils/session_manager.py`  

### **Orden de Implementaci√≥n Sugerido**

```
D√≠a 1:     Setup completo + schemas + fixtures
D√≠a 2-3:   Extraction (Track A)
D√≠a 4-5:   Preprocessing (Track A) + Frequency (Track B)
D√≠a 6-7:   Topics (Track C) + Skills (Track D)
D√≠a 8-10:  Visualizaciones (Track E)
D√≠a 11-13: Streamlit p√°ginas 1-2
D√≠a 14-16: Streamlit p√°ginas 3-5
D√≠a 17-19: Testing e integraci√≥n
D√≠a 20-22: Deploy y documentaci√≥n
```

¬øListo para empezar? Podemos comenzar con la **Fase 0: Setup** creando todos los archivos base. ¬øQuieres que generemos el setup inicial completo?